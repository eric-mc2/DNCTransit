{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from shapely.geometry import Point, shape\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "import s3fs\n",
    "from tqdm import tqdm\n",
    "from calendar import monthrange\n",
    "from datetime import datetime, timedelta\n",
    "from urllib3.exceptions import TimeoutError\n",
    "from requests.exceptions import ReadTimeout\n",
    "from typing import Generator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from zipfile import ZipFile\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rides\n",
    "TOTAL_RIDERSHIP_TABLE = \"6iiy-9s97\" # https://data.cityofchicago.org/Transportation/CTA-Ridership-Daily-Boarding-Totals/6iiy-9s97\n",
    "L_RIDERSHIP_TABLE = \"5neh-572f\" # https://data.cityofchicago.org/Transportation/CTA-Ridership-L-Station-Entries-Daily-Totals/5neh-572f\n",
    "BUS_RIDERSHIP_TABLE = \"jyb9-n7fm\" # https://data.cityofchicago.org/Transportation/CTA-Ridership-Bus-Routes-Daily-Totals-by-Route/jyb9-n7fm\n",
    "DIVVY_RIDERSHIP_TABLE = \"fg6s-gzvg\" # https://data.cityofchicago.org/Transportation/Divvy-Trips/fg6s-gzvg\n",
    "DIVVY_SUB_RIDERSHIP_TABLE = \"4am4-35ir\" # https://data.cityofchicago.org/Transportation/Divvy-Trips-Subscriber-Only/4am4-35ir\n",
    "UBER_RIDERSHIP_TABLE = \"n26f-ihde\" # https://data.cityofchicago.org/resource/n26f-ihde\n",
    "\n",
    "# Stations\n",
    "L_STATIONS_TABLE = \"8pix-ypme\" # https://data.cityofchicago.org/Transportation/CTA-System-Information-List-of-L-Stops/8pix-ypme\n",
    "DIVVY_STATIONS_TABLE = \"bbyy-e7gq\" # https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq\n",
    "BUS_ROUTES_TABLE = \"6uva-a5ei\" # https://data.cityofchicago.org/Transportation/CTA-Bus-Routes/6uva-a5ei\n",
    "BUS_STOPS_TABLE = \"pxug-u72f\" # https://data.cityofchicago.org/Transportation/CTA-Bus-Stops-Shapefile/pxug-u72f\n",
    "BUS_STOPS_TABLE = \"https://data.cityofchicago.org/download/pxug-u72f/application/x-zip-compressed\"\n",
    "BUILDINGS_TABLE = \"syp8-uezg\" # https://data.cityofchicago.org/Buildings/buildings/syp8-uezg\n",
    "TRACT_TABLE = \"https://data.cityofchicago.org/api/geospatial/5jrd-6zik?method=export&format=GeoJSON\"\n",
    "COMM_AREA_TABLE = \"https://data.cityofchicago.org/api/geospatial/cauq-8yn6?method=export&format=GeoJSON\"\n",
    "\n",
    "UNITED_CENTER = ((41,52,50,\"N\"), (87,40,27,\"W\")) # lat/lng\n",
    "MCCORMICK_PLACE = ((41,51,7,\"N\"), (87,36,58,\"W\"))\n",
    "OHARE_CENTROID = ((41,58,43,\"N\"), (87,54,17,\"W\"))\n",
    "MIDWAY_CENTROID = ((41,47,10,\"N\"), (87,45,9,\"W\"))\n",
    "\n",
    "YMD = \"%Y-%m-%d\"\n",
    "DNC_START = \"2024-08-19\"\n",
    "DNC_END = \"2024-08-22\"\n",
    "ERAS_START = \"2023-06-02\"\n",
    "ERAS_END = \"2023-06-04\"\n",
    "# TODO!\n",
    "# # Using the Taylor Swift Eras tour as a stand-in for the DNC since transit data isn't updated to August yet.\n",
    "# DNC_START = ERAS_START\n",
    "# DNC_END = ERAS_END\n",
    "DNC_START_ISO = datetime.strptime(DNC_START, YMD).isoformat()\n",
    "DNC_END_ISO = datetime.strptime(DNC_END, YMD).replace(hour=23, minute=59, second=59).isoformat()\n",
    "dnc_isoc = datetime.strptime(DNC_START, YMD).isocalendar()\n",
    "DNC_ISO_WEEK = \"{}-{}\".format(dnc_isoc.year, str(dnc_isoc.week).rjust(2,'0'))\n",
    "\n",
    "# I'm going to consider Phase 5 as the start point for the new normal.\n",
    "# This is \"Illinois Restored\", gatherings of 50+, including festivals, etc.\n",
    "COVID = {\"TIER_3\": \"2021-01-02\",\n",
    "         \"TIER_1\": \"2021-01-19\",\n",
    "         \"PHASE_4\": \"2021-02-02\",\n",
    "         \"PHASE_5\": \"2021-06-11\"}\n",
    "\n",
    "LOCAL_CRS = pyproj.CRS(\"EPSG:3435\") # NAD83 / Illinois East (ftUS)\n",
    "WORLD_CRS = pyproj.CRS(\"EPSG:4326\") # WGS84\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socrata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "client = Socrata(\n",
    "        \"data.cityofchicago.org\",\n",
    "        app_token=None,\n",
    "        timeout=600\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _soda_get_coltypes(resource):\n",
    "    \"\"\"Query basic table metadata\"\"\"\n",
    "    meta = client.get_metadata(resource)\n",
    "    colnames = [c['fieldName'] for c in meta['columns']]\n",
    "    coltypes = [c['dataTypeName'] for c in meta['columns']]\n",
    "    coltypes = {c: ct for c,ct in zip(colnames, coltypes)}\n",
    "    return coltypes\n",
    "    \n",
    "\n",
    "def _soda_fix_coltypes(df: pd.DataFrame, resource, aliases=None):\n",
    "    \"\"\"\n",
    "    Coerce pandas dtypes because SodaPy seems to return everything as strings.\n",
    "    \"\"\"\n",
    "    # Iterate through columns because query may subset columns\n",
    "    coltypes = _soda_get_coltypes(resource)\n",
    "    if aliases is not None:\n",
    "        for kv in aliases.split(','):\n",
    "            if ' AS ' in kv:\n",
    "                k = kv.split(' AS ')[0].strip()\n",
    "                v = kv.split(' AS ')[1].strip()\n",
    "                # Imperfectly extract column name from univariate expressions\n",
    "                # XXX: Assumes the function doesn't change the dtype\n",
    "                #      If we dont want that assumption we should only alias\n",
    "                #      if k exactly equals a column name ie no function\n",
    "                c = next(filter(lambda c: c in k, coltypes.keys()), None)\n",
    "                if c is not None:\n",
    "                    coltypes[v] = coltypes[c]\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in coltypes.keys():\n",
    "            continue # functions of columns might not preserve type!\n",
    "        elif coltypes[col] == 'calendar_date':\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        elif coltypes[col] == 'number':\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "    return df\n",
    "\n",
    "def _soda_to_df(data: Generator, has_header=True):\n",
    "    \"\"\"\n",
    "    Collect iterable of rows into a dataframe.\n",
    "    \"\"\"\n",
    "    if has_header:\n",
    "        header = list(next(data, {}).keys())\n",
    "        df = pd.DataFrame.from_records(data, columns=header)\n",
    "    else:\n",
    "        df = pd.DataFrame.from_records(data)\n",
    "    return df\n",
    "\n",
    "def soda_get_all(resource, has_header=True, **params):\n",
    "    \"\"\"Wrapper for client.get_all\"\"\"\n",
    "    if \"limit\" in params.keys():\n",
    "        # Sodapy doesn't allow limit with get_all :(\n",
    "        data = iter(client.get(resource, **params))\n",
    "    else:\n",
    "        data = client.get_all(resource, **params)\n",
    "    return (_soda_to_df(data, has_header)\n",
    "            .pipe(_soda_fix_coltypes, resource, params.get('select',None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_schema(df):\n",
    "    renamer = {\"pickup_centroid_location\": \"start_point\",\n",
    "               \"dropoff_centroid_location\": \"end_point\",\n",
    "               \"trip_start_timestamp\": \"start_date\"}\n",
    "    df = df.rename(columns=renamer)\n",
    "    if 'rides' in df.columns:\n",
    "        df['rides'] = pd.to_numeric(df['rides'], 'coerce')\n",
    "    if 'start_point' in df.columns:\n",
    "        df['start_point'] = df['start_point'].apply(shape)\n",
    "    if 'end_point' in df.columns:\n",
    "        df['end_point'] = df['end_point'].apply(shape)\n",
    "    if 'start_date' in df.columns:\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'],'coerce').dt.date\n",
    "    return df\n",
    "\n",
    "def soda_get_uber(start_date, end_date, **kwargs):\n",
    "    \"\"\"\n",
    "    Paginates soda_get_all over query months which is the largest timespan \n",
    "    the socrata endpoint can seem to handle.\n",
    "    \"\"\"\n",
    "    start_year = datetime.strptime(start_date, YMD).year\n",
    "    end_year = datetime.strptime(end_date, YMD).year\n",
    "    start_month = datetime.strptime(start_date, YMD).month\n",
    "    end_month = datetime.strptime(end_date, YMD).month\n",
    "    start_day = datetime.strptime(start_date, YMD).day\n",
    "    end_day = datetime.strptime(end_date, YMD).day\n",
    "\n",
    "    def _get_uber(start_iso, end_iso):\n",
    "        start_ymd = datetime.strftime(datetime.fromisoformat(start_iso), YMD)\n",
    "        cache_file = \"uber-dropoff-\" + start_ymd + \".csv\"\n",
    "        if os.path.exists(cache_file):\n",
    "            df = pd.read_csv(cache_file)\n",
    "        else:\n",
    "            where_dates = f\"trip_end_timestamp between '{where_start}' and '{where_end}'\"\n",
    "            print(where_dates)\n",
    "            df = soda_get_all(UBER_RIDERSHIP_TABLE, where=where_dates, **kwargs) \\\n",
    "                .pipe(uber_schema)\n",
    "            df.to_csv(cache_file, index=False)\n",
    "        return df\n",
    "\n",
    "    rides = []\n",
    "    for year in range(start_year, end_year+1):\n",
    "        year_start = start_month if year == start_year else 1\n",
    "        year_end = end_month if year == end_year else 12\n",
    "        for month in range(year_start, year_end + 1):\n",
    "            month_start = start_day if year == start_year and month == start_month else 1\n",
    "            month_end = end_day if year == end_year and month == end_month else monthrange(year, month)[1]\n",
    "            where_start = datetime(year, month, month_start).isoformat()\n",
    "            where_end = (datetime(year, month, month_end) + timedelta(hours=23, minutes=59, seconds=59)).isoformat()\n",
    "            try:\n",
    "                df = _get_uber(where_start, where_end)\n",
    "            except (TimeoutError, ReadTimeout):\n",
    "                df = []\n",
    "                for day in range(month_start, month_end+1):\n",
    "                    where_start = datetime(year, month, day).isoformat()\n",
    "                    where_end = (datetime(year, month, day) + timedelta(hours=23, minutes=59, seconds=59)).isoformat()\n",
    "                    df.append(_get_uber(where_start, where_end))\n",
    "                df = pd.concat(df, ignore_index=True)\n",
    "            rides.append(df)\n",
    "    return pd.concat(rides, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bucket_paths() -> Generator:\n",
    "    \"\"\"\n",
    "    Returns pairs of s3://filepaths and membername.csv in the bucket.\n",
    "    \n",
    "    Handles:\n",
    "    - ignoring irrelevant files in the bucket, like the index.html page\n",
    "    - ignoring irrelevant member files like system files in the ZIP files\n",
    "    - including multiple valid CSVs per ZIP like multiple quarters\n",
    "    - some station_ids are normalized and some aren't\n",
    "    \"\"\"\n",
    "    s3_paths = [f\"s3://{x}\" for x in s3.glob('divvy-tripdata/*.zip')]\n",
    "    csv_filter = lambda x: x.endswith('.csv') and 'MACOSX' not in x\n",
    "    shp_filter = lambda x: x.endswith('.shp.zip') and 'MACOSX' not in x\n",
    "    trip_filter = lambda x: 'trip' in basename(x.lower()) and csv_filter(x)\n",
    "    station_filter = lambda x: 'station' in basename(x.lower()) and (csv_filter(x) or shp_filter(x))\n",
    "    for s3_path in s3_paths:\n",
    "        with s3.open(s3_path, mode='rb') as s3f:\n",
    "            with ZipFile(s3f) as zf:\n",
    "                station_path = filter(station_filter, zf.namelist())\n",
    "                station_path = sorted(station_path, key=csv_filter)\n",
    "                station_path = station_path.pop() if station_path else None\n",
    "                for csv_path in filter(trip_filter, zf.namelist()):\n",
    "                    yield (s3_path, csv_path, station_path)\n",
    "                if not any(map(csv_filter, zf.namelist())):\n",
    "                    print(f\"WARNING: Did not find csv in {s3_path}\")\n",
    "\n",
    "# XXX: Hacky way to cache the generator since it takes a while just to query paths.\n",
    "bucket_paths = list(_get_bucket_paths())\n",
    "def get_bucket_paths():\n",
    "    return iter(bucket_paths)\n",
    "\n",
    "def station_schema(df):\n",
    "    \"\"\"Helper function to unify schema drift.\"\"\"\n",
    "    return (\n",
    "        df.rename(columns= lambda x: x.lower())\n",
    "          .rename(columns = {\n",
    "            \"lat\": \"latitude\",\n",
    "            \"long\": \"longitude\",\n",
    "            \"id_list\": \"station_id\",\n",
    "            \"id\": \"station_id\",\n",
    "            \"online date\": \"online_date\"\n",
    "        }))\n",
    "\n",
    "def trip_schema(df):\n",
    "    \"\"\"Helper function to unify schema drift.\"\"\"\n",
    "    return (\n",
    "        df.rename(columns={\n",
    "            '01 - Rental Details Rental ID': 'ride_id',\n",
    "            '01 - Rental Details Local Start Time': 'start_time',\n",
    "            '01 - Rental Details Local End Time': 'end_time', \n",
    "            '01 - Rental Details Bike ID': 'bike_id',\n",
    "            '01 - Rental Details Duration In Seconds Uncapped': 'tripduration',\n",
    "            '03 - Rental Start Station ID': 'start_station_id',\n",
    "            '03 - Rental Start Station Name': 'start_station_name',\n",
    "            '02 - Rental End Station ID': 'end_station_id',\n",
    "            '02 - Rental End Station Name': 'end_station_name',\n",
    "            'User Type': 'user_type',\n",
    "            'Member Gender': 'gender',\n",
    "            '05 - Member Details Member Birthday Year': 'birthyear'\n",
    "            })\n",
    "          .rename(columns= lambda x: x.lower())\n",
    "          .rename(columns = {\n",
    "            'from_lng': 'start_lng',\n",
    "            'from_lat': 'start_lat',\n",
    "            'to_lng': 'end_lng',\n",
    "            'to_lat': 'end_lat',\n",
    "            'trip_id': 'ride_id',\n",
    "            'from_station_id': 'start_station_id',\n",
    "            'to_station_id': 'end_station_id',\n",
    "            'from_station_name': 'start_station_name',\n",
    "            'to_station_name': 'end_station_name',\n",
    "            'starttime': 'start_time',\n",
    "            'stoptime': 'end_time',\n",
    "            'stop_time': 'end_time',\n",
    "            'started_at': 'start_time',\n",
    "            'ended_at': 'end_time',\n",
    "            'bikeid': 'bike_id',\n",
    "            'tripduration': 'trip_duration',\n",
    "            'usertype': 'user_type',\n",
    "            'member_casual': 'user_type',\n",
    "            'duration': 'trip_duration',\n",
    "            'birthday': 'birthyear',\n",
    "          }))\n",
    "\n",
    "def read_trip_file(fp):\n",
    "    \"\"\"Helper function to unify schema drift.\"\"\"\n",
    "    df = pd.read_csv(fp).pipe(trip_schema)\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'], errors='coerce')\n",
    "    if 'start_lng' in df.columns:\n",
    "        df['start_geometry'] = gpd.points_from_xy(df['start_lng'],df['start_lat'],crs=WORLD_CRS)\n",
    "        df['end_geometry'] = gpd.points_from_xy(df['end_lng'],df['end_lat'],crs=WORLD_CRS)\n",
    "    return df\n",
    "\n",
    "def s3_bike_trips(min_year, max_year):\n",
    "    rides = []\n",
    "    for zip_path, trip_path, station_path in tqdm(get_bucket_paths()):\n",
    "        pats = [r\"(\\d{4}).*-divvy-tripdata.zip\", r\"Divvy_.*Trips_(\\d{4}).*.zip\"]\n",
    "        matches = filter(None, map(lambda z: re.search(z, zip_path), pats))\n",
    "        year = int(next(map(lambda y: y.group(1), matches)))\n",
    "        if min_year <= year and year <= max_year:\n",
    "            with s3.open(zip_path, mode='rb') as s3f, ZipFile(s3f) as zf, zf.open(trip_path) as tripf:\n",
    "                df = read_trip_file(tripf).assign(vintage=basename(zip_path))\n",
    "                rides.append(df)\n",
    "    return pd.concat(rides, ignore_index=True)\n",
    "\n",
    "def s3_bike_stations(fp):\n",
    "    \"\"\"Helper function to unify schema drift.\"\"\"\n",
    "    if fp.name.endswith(\".csv\"):\n",
    "        df = pd.read_csv(fp).pipe(station_schema)\n",
    "    else:\n",
    "        df = gpd.read_file(fp).pipe(station_schema)\n",
    "    keep_cols = ['station_id','name','latitude','longitude','geometry']\n",
    "    return df.filter(keep_cols)\n",
    "\n",
    "def s3_point_gdf(df, lng_col, lat_col, loc_col):\n",
    "    \"\"\"Helper function to compute projected geometries.\"\"\"\n",
    "    crs = df.crs if isinstance(df, gpd.GeoDataFrame) else WORLD_CRS\n",
    "    if loc_col not in df.columns:\n",
    "        df = df.assign(**{loc_col: gpd.points_from_xy(df[lng_col], df[lat_col], crs=crs)})\n",
    "    return gpd.GeoDataFrame(df, geometry=loc_col, crs=crs)\n",
    "\n",
    "def project_gdf(gdf, crs=LOCAL_CRS):\n",
    "    return gdf.to_crs(crs)\n",
    "\n",
    "# def geofilter(df, lng_col, lat_col, loc_col, point, dist):\n",
    "#     \"\"\"Helper function to filter rows within dist meters of point.\"\"\"\n",
    "#     gdf = create_geodf(df, lng_col, lat_col, loc_col)\n",
    "#     point = gpd.GeoSeries([point], crs=WORLD_CRS).to_crs(gdf.crs)\n",
    "#     return gdf[gdf.geometry.dwithin(point[0], dist)]\n",
    "\n",
    "# def station_geofilter(trip_df, station_df, point, dist):\n",
    "#     \"\"\"Top-level function to sptially filter normalized dataframes.\"\"\"\n",
    "#     filtered_stations = geofilter(station_df, 'longitude', 'latitude', 'geometry', point, dist)[['station_id']]\n",
    "#     df_from = trip_df.merge(filtered_stations, left_on='start_station_id', right_on='station_id')\n",
    "#     df_to = trip_df.merge(filtered_stations, left_on='end_station_id', right_on='station_id')\n",
    "#     return pd.concat([df_from, df_to], ignore_index=True).drop(columns=['station_id']).drop_duplicates('ride_id')\n",
    "    \n",
    "# def trip_geofilter(trip_df, station_df, point, dist):\n",
    "#     \"\"\"\n",
    "#     Top-level function to spatially filter single denormalized dataframe.\n",
    "#     Takes station_df as a last resort when denormalized trip_df is incomplete.\n",
    "#     \"\"\"\n",
    "#     if 'start_lng' in trip_df.columns:\n",
    "#         df_from = geofilter(trip_df, 'start_lng', 'start_lat', 'start_loc', point, dist)\n",
    "#         df_to = geofilter(trip_df, 'end_lng', 'end_lat', 'end_loc', point, dist)\n",
    "#         return pd.concat([df_from, df_to], ignore_index=True).drop_duplicates('ride_id')\n",
    "#     else:\n",
    "#         # Coerce the dtypes to match to avoid a warning.\n",
    "#         # In the bad case when dtypes dont match, this should still return empty.\n",
    "#         # In the good case when dtypes already match, it shouldn't affect outcome either.\n",
    "#         trip_df['start_station_id'] = trip_df['start_station_id'].astype(station_df['station_id'].dtype)\n",
    "#         trip_df['end_station_id'] = trip_df['end_station_id'].astype(station_df['station_id'].dtype)\n",
    "#         return station_geofilter(trip_df, station_df, point, dist)\n",
    "    \n",
    "def meter_to_foot(x):\n",
    "    return x * 3.281\n",
    "\n",
    "def agg_bike_trips(rides):\n",
    "    rides['start_date'] = rides['start_time'].dt.date\n",
    "    rides['end_date'] = rides['end_time'].dt.date\n",
    "    id_cols = ['station_id','date','vintage'] \n",
    "    id_cols += ['geometry'] if any('geometry' in x for x in rides.columns) else []\n",
    "    start_rides = rides.rename(columns=lambda x: x.replace('start_',''))[id_cols]\n",
    "    end_rides = rides.rename(columns=lambda x: x.replace('end_',''))[id_cols]\n",
    "    rides = pd.concat([start_rides, end_rides], ignore_index=True)\n",
    "    rides = rides.value_counts(id_cols).rename('rides').reset_index()\n",
    "    rides['date'] = pd.to_datetime(rides['date'])\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = soda_get_all(L_STATIONS_TABLE, select=\"stop_id, direction_id, stop_name, station_name, map_id, location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stations['location'].apply(shape) # is not working!\n",
    "train_stations['latitude'] = train_stations['location'].apply(lambda x: x['latitude'])\n",
    "train_stations['longitude'] = train_stations['location'].apply(lambda x: x['longitude'])\n",
    "train_stations['geometry'] = gpd.points_from_xy(train_stations['longitude'], train_stations['latitude'])\n",
    "train_stations = train_stations.drop(columns=['location', 'latitude', 'longitude'])\n",
    "train_stations = gpd.GeoDataFrame(train_stations, geometry='geometry',crs=WORLD_CRS)\n",
    "# nb: Each train station is represented as two \"stops\" per station: one in each direction.\n",
    "#     For our purposes, since we don't model the direction of travel, we will drop the redundant \"stop\".\n",
    "train_stations = train_stations.drop_duplicates(['station_name','map_id','geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bus Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_routes =  soda_get_all(BUS_ROUTES_TABLE, select=\"the_geom, route, name\")\n",
    "bus_routes['geometry'] = bus_routes['the_geom'].apply(shape)\n",
    "bus_routes = bus_routes.drop(columns='the_geom')\n",
    "bus_routes = gpd.GeoDataFrame(bus_routes, geometry='geometry',crs=WORLD_CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops = gpd.read_file(BUS_STOPS_TABLE, columns=['ROUTESSTPG', 'geometry','PUBLIC_NAM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops['ROUTESSTPG'] = bus_stops['ROUTESSTPG'].str.split(',')\n",
    "bus_stops = bus_stops.explode('ROUTESSTPG').rename(columns={'ROUTESSTPG':'route'})\n",
    "# nb: Compared to train stations, bus stop pairs on opposite sides of the street\n",
    "#     aren't AS CLEANLY paired in the dataset. Though we could spatially join them\n",
    "#     as 1-nearest-neighbor if we really wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metra Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metra does not provide machine-readable ridership reports. They have bar graphs of weekly total ridership and monthly ridership by line.\n",
    "\n",
    "https://metra.com/ridership-reports\n",
    "\n",
    "TODO!\n",
    "\n",
    "But actually the Regional Transit Authority does provide machine-readable monthly ridership by line.\n",
    "\n",
    "https://rtams.org/media/datasets/metra-ridership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bike Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:15,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "bike_stations = []\n",
    "for zip_path, _, station_path in tqdm(filter(lambda x: x[2], get_bucket_paths())):\n",
    "    with (s3.open(zip_path, mode='rb') as s3f, ZipFile(s3f) as zf, zf.open(station_path) as stationf):\n",
    "        df = s3_bike_stations(stationf).assign(vintage=basename(zip_path))\n",
    "        gdf = s3_point_gdf(df, \"longitude\",\"latitude\",\"geometry\")\n",
    "        bike_stations.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some data is projected and some is not so we need to reproject to combine.\n",
    "# from collections import Counter\n",
    "# Counter([x.crs.to_authority() for x in bike_stations])\n",
    "# # Actually only one of the files was projected so we will un-project everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations = [project_gdf(x, WORLD_CRS) for x in bike_stations]\n",
    "# Now we can combine.\n",
    "# note: station_id are floats on all normalized tables!\n",
    "bike_stations = pd.concat(bike_stations, ignore_index=True).drop_duplicates()\n",
    "bike_stations = bike_stations.drop(columns=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uber Panel\n",
    "\n",
    "(tracts and community areas since this is anonymized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_points = gpd.read_file(TRACT_TABLE).filter(['geoid10','geometry']).drop_duplicates()\n",
    "tract_points['geoid10'] = pd.to_numeric(tract_points['geoid10'])\n",
    "tract_points['geometry'] = tract_points['geometry'].to_crs(LOCAL_CRS)\n",
    "tract_points['centroid'] = tract_points['geometry'].centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_points = gpd.read_file(COMM_AREA_TABLE).filter(['area_num_1','geometry']).drop_duplicates()\n",
    "comm_points['geometry'] = comm_points['geometry'].to_crs(LOCAL_CRS)\n",
    "comm_points['centroid'] = comm_points['geometry'].centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dms_to_decimal(degrees, minutes, seconds, direction):\n",
    "    decimal = degrees + minutes / 60 + seconds / 3600\n",
    "    if direction in ['S', 'W']:  # South and West should be negative\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "uc_xy = dms_to_decimal(*UNITED_CENTER[1]), dms_to_decimal(*UNITED_CENTER[0])\n",
    "uc_xy = Point(*uc_xy) # lng/lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_buildings = soda_get_all(BUILDINGS_TABLE, \n",
    "            where=f\"within_circle(the_geom, {uc_xy.y}, {uc_xy.x}, 250)\")\n",
    "uc_buildings['geometry'] = uc_buildings['the_geom'].apply(shape)\n",
    "uc_buildings = gpd.GeoDataFrame(uc_buildings, geometry='geometry', crs=WORLD_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verifying this is the right building\n",
    "# print(uc_buildings.bldg_name1)\n",
    "# ax = uc_buildings.iloc[0:1].plot()\n",
    "# uc_buildings.iloc[1:].plot(ax=ax,color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_building = uc_buildings[uc_buildings['bldg_name1'] == 'UNITED CENTER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCormick Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_xy = dms_to_decimal(*MCCORMICK_PLACE[1]), dms_to_decimal(*MCCORMICK_PLACE[0])\n",
    "mp_xy = Point(*mp_xy) # lng/lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_buildings = soda_get_all(BUILDINGS_TABLE, \n",
    "        where=f\"within_circle(the_geom, {mp_xy.y}, {mp_xy.x}, 250)\")\n",
    "mp_buildings['geometry'] = mp_buildings['the_geom'].apply(shape)\n",
    "mp_buildings = gpd.GeoDataFrame(mp_buildings, geometry='geometry', crs=WORLD_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verifying this is the right building\n",
    "# print(mp_buildings.bldg_name1)\n",
    "# ax = mp_buildings.iloc[0:1].plot()\n",
    "# mp_buildings.iloc[1:].plot(ax=ax,color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_building = mp_buildings[mp_buildings['bldg_name1'] == 'HYATT REGENCY MCCORMICK PLACE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and bus stops are easy to look up by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aiports = (train_stations.station_name == \"O'Hare\")|(train_stations.station_name == \"Midway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb: CTA busses don't go directly into O'Hare, nor even to the adjacent Mixed Modal Transit center.\n",
    "bus_airports = bus_stops['PUBLIC_NAM']==\"Midway Orange Line Station\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rideshare pickups are anonymized to census area so we can't use building catchements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_xy = dms_to_decimal(*OHARE_CENTROID[1]), dms_to_decimal(*OHARE_CENTROID[0])\n",
    "oh_xy = Point(*oh_xy) # lng/lat\n",
    "oh_tract = tract_points.set_index('geoid10').geometry.contains(gpd.GeoSeries([oh_xy], crs=WORLD_CRS).to_crs(LOCAL_CRS).iloc[0])\n",
    "oh_comm = comm_points.set_index('area_num_1').geometry.contains(gpd.GeoSeries([oh_xy], crs=WORLD_CRS).to_crs(LOCAL_CRS).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdw_xy = dms_to_decimal(*MIDWAY_CENTROID[1]), dms_to_decimal(*MIDWAY_CENTROID[0])\n",
    "mdw_xy = Point(*mdw_xy) # lng/lat\n",
    "mdw_tract = tract_points.set_index('geoid10').geometry.contains(gpd.GeoSeries([mdw_xy], crs=WORLD_CRS).to_crs(LOCAL_CRS).iloc[0])\n",
    "mdw_comm = comm_points.set_index('area_num_1').geometry.contains(gpd.GeoSeries([mdw_xy], crs=WORLD_CRS).to_crs(LOCAL_CRS).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_tracts = oh_tract | mdw_tract\n",
    "air_comms = oh_comm | mdw_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_buffers(df: pd.DataFrame, geom:gpd.GeoSeries, geom_prefix:str, dists:list[int]):\n",
    "    building_proj = geom.geometry.to_crs(LOCAL_CRS)\n",
    "    buffers = [building_proj.buffer(meter_to_foot(d)) for d in dists]\n",
    "    df_proj = df.geometry.to_crs(LOCAL_CRS)\n",
    "    codes = {f\"{geom_prefix}_{d}\": df_proj.intersects(b[0]) * 1.0 for d,b in zip(dists,buffers)}\n",
    "    return df.assign(**codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = train_stations.pipe(code_buffers, uc_building, \"uc\", [400,800,1600])\n",
    "bike_stations = bike_stations.pipe(code_buffers, uc_building, \"uc\", [400,800,1600])\n",
    "bus_stops = bus_stops.pipe(code_buffers, uc_building, \"uc\", [400,800,1600])\n",
    "\n",
    "train_stations = train_stations.pipe(code_buffers, mp_building, \"mp\", [400,800,1600])\n",
    "bike_stations = bike_stations.pipe(code_buffers, mp_building, \"mp\", [400,800,1600])\n",
    "bus_stops = bus_stops.pipe(code_buffers, mp_building, \"mp\", [400,800,1600])\n",
    "\n",
    "train_stations['airport'] = train_aiports\n",
    "bus_stops['airport'] = bus_airports\n",
    "bike_stations['airport'] = False\n",
    "\n",
    "# TODO! Code tracts and community eareas??\n",
    "# Also maybe code the UC and MP community areas in the other things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only have ridership by bus route, not bus stop, so to aggregate stops to routes,\n",
    "# we'll compute the mean number of stops within the buffer per route.\n",
    "buffer_cols = ['uc_400','uc_800','uc_1600','mp_400','mp_800','mp_1600','airport']\n",
    "bus_stops_uc = bus_stops.groupby('route',as_index=False)[buffer_cols].mean()\n",
    "bus_routes = bus_routes.merge(bus_stops_uc, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_panels(bus=None, train=None, bike=None):\n",
    "    if bus is not None:\n",
    "        bus = bus.rename(columns={'route':'id'}).assign(transit='bus', tid=\"bus_\"+bus['route'].astype(str))\n",
    "    if train is not None:\n",
    "        train = train.rename(columns={'map_id':'id'}).assign(transit='train', tid=\"train_\"+train['map_id'].astype(str))\n",
    "    if bike is not None:\n",
    "        bike = bike.rename(columns={'station_id':'id'}).assign(transit='bike', tid=\"bike_\"+bike['station_id'].astype(str))\n",
    "    panel = pd.concat(filter(lambda x: x is not None and not x.empty, [bus,train,bike]), ignore_index=True, join='inner')\n",
    "    panel['lat'] = gpd.GeoSeries(panel.geometry).to_crs(LOCAL_CRS).centroid.y\n",
    "    panel['long'] = gpd.GeoSeries(panel.geometry).to_crs(LOCAL_CRS).centroid.x\n",
    "    panel = panel.drop(columns=['geometry'])\n",
    "    return panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_panel = combine_panels(bus_routes, train_stations, bike_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce(df, left, right, coalesced):\n",
    "    predicate = df[left].isna()\n",
    "    df[coalesced] = df[left]\n",
    "    df[predicate][coalesced] = df[predicate][right]\n",
    "    return df.drop(columns=[left,right])\n",
    "\n",
    "def get_rides_panel(ctl_start, ctl_end, trt_start, trt_end):\n",
    "    soql_where_date = f\"\"\"(('{trt_start}' <= date) AND (date <= '{trt_end}'))\n",
    "                OR (('{ctl_start}' <= date) AND (date <= '{ctl_end}'))\"\"\"\n",
    "    def _pd_where_date(x):\n",
    "        dts = pd.to_datetime(pd.Series([ctl_start, ctl_end, trt_start, trt_end]))\n",
    "        return ((dts[0] <= x['date']) & (x['date'] <= dts[1])) | \\\n",
    "                ((dts[2] <= x['date']) & (x['date'] <= dts[3]))\n",
    "    \n",
    "    train_rides = soda_get_all(L_RIDERSHIP_TABLE, \n",
    "                            select=\"station_id,date,daytype,rides\",\n",
    "                            where=soql_where_date) \\\n",
    "                .merge(train_stations, left_on='station_id', right_on='map_id')\n",
    "\n",
    "    bus_rides = soda_get_all(BUS_RIDERSHIP_TABLE, \n",
    "                            select=\"route,date,daytype,rides\",\n",
    "                            where=soql_where_date) \\\n",
    "                .merge(bus_routes, on='route')\n",
    "\n",
    "    bike_rides = s3_bike_trips(datetime.fromisoformat(ctl_start).year, \n",
    "                            datetime.fromisoformat(trt_end).year) \\\n",
    "                .pipe(agg_bike_trips) \\\n",
    "                .loc[_pd_where_date]\n",
    "    # Half of the bike rides are already denormalized and don't need bike-stations\n",
    "    # So we do a left-join and coalesce to get the missing geometries\n",
    "    bike_rides = bike_rides.merge(bike_stations.assign(station_id=bike_stations.station_id.astype(str)),\n",
    "                                   on=['station_id','vintage'], how='left')\n",
    "    bike_rides = bike_rides.pipe(coalesce, 'geometry_x','geometry_y','geometry')\n",
    "    # Bikes don't have the daytype column so we'll impute it\n",
    "    daytypes = pd.concat([train_rides[['date','daytype']],bus_rides[['date','daytype']]],ignore_index=True)\n",
    "    daytypes = daytypes.groupby('date')['daytype'].first()\n",
    "    bike_rides['daytype'] = bike_rides['date'].map(daytypes)\n",
    "    \n",
    "    rides = combine_panels(bus_rides, train_rides, bike_rides)\n",
    "    rides['DNC'] = (trt_start <= rides['date']) & (rides['date'] <= trt_end)\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_weekly(df, datecol, groupcols, agg_func):\n",
    "    iso = df[datecol].dt.isocalendar()\n",
    "    weeks = iso.week.astype(str).str.pad(2,'left','0')\n",
    "    df = df.assign(**{datecol: iso.year.astype(str) + '-' + weeks})\n",
    "    return df.groupby(groupcols + [datecol], as_index=False).agg(agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "service_date=%{x}<br>total_rides=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2024-01",
          "2024-02",
          "2024-03",
          "2024-04",
          "2024-05",
          "2024-06",
          "2024-07",
          "2024-08",
          "2024-09",
          "2024-10",
          "2024-11",
          "2024-12",
          "2024-13",
          "2024-14",
          "2024-15",
          "2024-16",
          "2024-17",
          "2024-18",
          "2024-19",
          "2024-20",
          "2024-21",
          "2024-22",
          "2024-23",
          "2024-24",
          "2024-25",
          "2024-26",
          "2024-27",
          "2024-28",
          "2024-29",
          "2024-30",
          "2024-31",
          "2024-32",
          "2024-33",
          "2024-34",
          "2024-35"
         ],
         "xaxis": "x",
         "y": [
          3865549,
          4819104,
          4097553,
          5479513,
          5743622,
          5742722,
          5696812,
          5564186,
          5807104,
          5690993,
          6215810,
          5704642,
          5258992,
          5800357,
          6052766,
          6167809,
          6104930,
          6277083,
          6188309,
          6303400,
          6317347,
          5780474,
          6328414,
          6064550,
          5936238,
          6121355,
          5509128,
          5966411,
          6127564,
          6149247,
          6372243,
          6148046,
          5958152,
          5670432,
          5964712
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "gray",
           "dash": "dash"
          },
          "type": "line",
          "x0": "2024-34",
          "x1": "2024-34",
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "service_date"
         },
         "type": "category"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "total_rides"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cta_rides = soda_get_all(TOTAL_RIDERSHIP_TABLE, \n",
    "             has_header=True,\n",
    "             select=\"service_date, total_rides\", \n",
    "             where=\"service_date between '2024-01-01T00:00:00' and '2024-12-31T00:00:00'\")\n",
    "cta_weekly = cta_rides.pipe(agg_weekly, 'service_date', [], {'total_rides':'sum'})\n",
    "fig = px.line(cta_weekly, x='service_date', y='total_rides')\n",
    "fig.add_vline(x=DNC_ISO_WEEK, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis=dict(type='category'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:55,  1.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "date=%{x}<br>rides=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2024-01",
          "2024-02",
          "2024-03",
          "2024-04",
          "2024-05",
          "2024-06",
          "2024-07",
          "2024-08",
          "2024-09",
          "2024-10",
          "2024-11",
          "2024-12",
          "2024-13",
          "2024-14",
          "2024-15",
          "2024-16",
          "2024-17",
          "2024-18",
          "2024-19",
          "2024-20",
          "2024-21",
          "2024-22",
          "2024-23",
          "2024-24",
          "2024-25",
          "2024-26",
          "2024-27",
          "2024-28",
          "2024-29",
          "2024-30",
          "2024-31",
          "2024-32",
          "2024-33",
          "2024-34",
          "2024-35",
          "2024-36",
          "2024-37",
          "2024-38",
          "2024-39",
          "2024-40"
         ],
         "xaxis": "x",
         "y": [
          70067,
          55022,
          28483,
          56738,
          86189,
          94901,
          81161,
          104135,
          131797,
          104779,
          135394,
          96589,
          114312,
          90901,
          195388,
          162232,
          171317,
          207876,
          204600,
          239904,
          239198,
          215255,
          264100,
          271722,
          283368,
          257680,
          269090,
          255615,
          295901,
          285368,
          290714,
          291952,
          249186,
          280080,
          282002,
          293253,
          321238,
          295904,
          260292,
          39851
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "gray",
           "dash": "dash"
          },
          "type": "line",
          "x0": "2024-34",
          "x1": "2024-34",
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "date"
         },
         "type": "category"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "rides"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bike_rides = s3_bike_trips(2024,2024).pipe(agg_bike_trips).sort_values('date')\n",
    "bike_weekly = bike_rides.pipe(agg_weekly, 'date', [], {'rides':'sum'})\n",
    "fig = px.line(bike_weekly, x='date', y='rides')\n",
    "fig.add_vline(x=DNC_ISO_WEEK, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis=dict(type='category'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_end_timestamp between '2024-01-16T00:00:00' and '2024-01-31T23:59:59'\n",
      "trip_end_timestamp between '2024-02-01T00:00:00' and '2024-02-29T23:59:59'\n",
      "trip_end_timestamp between '2024-03-01T00:00:00' and '2024-03-31T23:59:59'\n",
      "trip_end_timestamp between '2024-03-01T00:00:00' and '2024-03-01T23:59:59'\n",
      "trip_end_timestamp between '2024-03-02T00:00:00' and '2024-03-02T23:59:59'\n",
      "trip_end_timestamp between '2024-03-03T00:00:00' and '2024-03-03T23:59:59'\n",
      "trip_end_timestamp between '2024-03-04T00:00:00' and '2024-03-04T23:59:59'\n",
      "trip_end_timestamp between '2024-03-05T00:00:00' and '2024-03-05T23:59:59'\n",
      "trip_end_timestamp between '2024-03-06T00:00:00' and '2024-03-06T23:59:59'\n",
      "trip_end_timestamp between '2024-03-07T00:00:00' and '2024-03-07T23:59:59'\n",
      "trip_end_timestamp between '2024-03-08T00:00:00' and '2024-03-08T23:59:59'\n",
      "trip_end_timestamp between '2024-03-09T00:00:00' and '2024-03-09T23:59:59'\n",
      "trip_end_timestamp between '2024-03-10T00:00:00' and '2024-03-10T23:59:59'\n",
      "trip_end_timestamp between '2024-03-11T00:00:00' and '2024-03-11T23:59:59'\n",
      "trip_end_timestamp between '2024-03-12T00:00:00' and '2024-03-12T23:59:59'\n",
      "trip_end_timestamp between '2024-03-13T00:00:00' and '2024-03-13T23:59:59'\n",
      "trip_end_timestamp between '2024-03-14T00:00:00' and '2024-03-14T23:59:59'\n",
      "trip_end_timestamp between '2024-03-15T00:00:00' and '2024-03-15T23:59:59'\n",
      "trip_end_timestamp between '2024-03-16T00:00:00' and '2024-03-16T23:59:59'\n",
      "trip_end_timestamp between '2024-03-17T00:00:00' and '2024-03-17T23:59:59'\n",
      "trip_end_timestamp between '2024-03-18T00:00:00' and '2024-03-18T23:59:59'\n",
      "trip_end_timestamp between '2024-03-19T00:00:00' and '2024-03-19T23:59:59'\n",
      "trip_end_timestamp between '2024-03-20T00:00:00' and '2024-03-20T23:59:59'\n",
      "trip_end_timestamp between '2024-03-21T00:00:00' and '2024-03-21T23:59:59'\n",
      "trip_end_timestamp between '2024-03-22T00:00:00' and '2024-03-22T23:59:59'\n",
      "trip_end_timestamp between '2024-03-23T00:00:00' and '2024-03-23T23:59:59'\n",
      "trip_end_timestamp between '2024-03-24T00:00:00' and '2024-03-24T23:59:59'\n",
      "trip_end_timestamp between '2024-03-25T00:00:00' and '2024-03-25T23:59:59'\n",
      "trip_end_timestamp between '2024-03-26T00:00:00' and '2024-03-26T23:59:59'\n",
      "trip_end_timestamp between '2024-03-27T00:00:00' and '2024-03-27T23:59:59'\n",
      "trip_end_timestamp between '2024-03-28T00:00:00' and '2024-03-28T23:59:59'\n",
      "trip_end_timestamp between '2024-03-29T00:00:00' and '2024-03-29T23:59:59'\n",
      "trip_end_timestamp between '2024-03-30T00:00:00' and '2024-03-30T23:59:59'\n",
      "trip_end_timestamp between '2024-03-31T00:00:00' and '2024-03-31T23:59:59'\n",
      "trip_end_timestamp between '2024-04-01T00:00:00' and '2024-04-30T23:59:59'\n",
      "trip_end_timestamp between '2024-04-01T00:00:00' and '2024-04-01T23:59:59'\n",
      "trip_end_timestamp between '2024-04-02T00:00:00' and '2024-04-02T23:59:59'\n",
      "trip_end_timestamp between '2024-04-03T00:00:00' and '2024-04-03T23:59:59'\n",
      "trip_end_timestamp between '2024-04-04T00:00:00' and '2024-04-04T23:59:59'\n",
      "trip_end_timestamp between '2024-04-05T00:00:00' and '2024-04-05T23:59:59'\n",
      "trip_end_timestamp between '2024-04-06T00:00:00' and '2024-04-06T23:59:59'\n",
      "trip_end_timestamp between '2024-04-07T00:00:00' and '2024-04-07T23:59:59'\n",
      "trip_end_timestamp between '2024-04-08T00:00:00' and '2024-04-08T23:59:59'\n",
      "trip_end_timestamp between '2024-04-09T00:00:00' and '2024-04-09T23:59:59'\n",
      "trip_end_timestamp between '2024-04-10T00:00:00' and '2024-04-10T23:59:59'\n",
      "trip_end_timestamp between '2024-04-11T00:00:00' and '2024-04-11T23:59:59'\n",
      "trip_end_timestamp between '2024-04-12T00:00:00' and '2024-04-12T23:59:59'\n",
      "trip_end_timestamp between '2024-04-13T00:00:00' and '2024-04-13T23:59:59'\n",
      "trip_end_timestamp between '2024-04-14T00:00:00' and '2024-04-14T23:59:59'\n",
      "trip_end_timestamp between '2024-04-15T00:00:00' and '2024-04-15T23:59:59'\n",
      "trip_end_timestamp between '2024-04-16T00:00:00' and '2024-04-16T23:59:59'\n",
      "trip_end_timestamp between '2024-04-17T00:00:00' and '2024-04-17T23:59:59'\n",
      "trip_end_timestamp between '2024-04-18T00:00:00' and '2024-04-18T23:59:59'\n",
      "trip_end_timestamp between '2024-04-19T00:00:00' and '2024-04-19T23:59:59'\n",
      "trip_end_timestamp between '2024-04-20T00:00:00' and '2024-04-20T23:59:59'\n",
      "trip_end_timestamp between '2024-04-21T00:00:00' and '2024-04-21T23:59:59'\n",
      "trip_end_timestamp between '2024-04-22T00:00:00' and '2024-04-22T23:59:59'\n",
      "trip_end_timestamp between '2024-04-23T00:00:00' and '2024-04-23T23:59:59'\n",
      "trip_end_timestamp between '2024-04-24T00:00:00' and '2024-04-24T23:59:59'\n",
      "trip_end_timestamp between '2024-04-25T00:00:00' and '2024-04-25T23:59:59'\n",
      "trip_end_timestamp between '2024-04-26T00:00:00' and '2024-04-26T23:59:59'\n",
      "trip_end_timestamp between '2024-04-27T00:00:00' and '2024-04-27T23:59:59'\n",
      "trip_end_timestamp between '2024-04-28T00:00:00' and '2024-04-28T23:59:59'\n",
      "trip_end_timestamp between '2024-04-29T00:00:00' and '2024-04-29T23:59:59'\n",
      "trip_end_timestamp between '2024-04-30T00:00:00' and '2024-04-30T23:59:59'\n",
      "trip_end_timestamp between '2024-05-01T00:00:00' and '2024-05-31T23:59:59'\n",
      "trip_end_timestamp between '2024-05-01T00:00:00' and '2024-05-01T23:59:59'\n",
      "trip_end_timestamp between '2024-05-02T00:00:00' and '2024-05-02T23:59:59'\n",
      "trip_end_timestamp between '2024-05-03T00:00:00' and '2024-05-03T23:59:59'\n",
      "trip_end_timestamp between '2024-05-04T00:00:00' and '2024-05-04T23:59:59'\n",
      "trip_end_timestamp between '2024-05-05T00:00:00' and '2024-05-05T23:59:59'\n",
      "trip_end_timestamp between '2024-05-06T00:00:00' and '2024-05-06T23:59:59'\n",
      "trip_end_timestamp between '2024-05-07T00:00:00' and '2024-05-07T23:59:59'\n",
      "trip_end_timestamp between '2024-05-08T00:00:00' and '2024-05-08T23:59:59'\n",
      "trip_end_timestamp between '2024-05-09T00:00:00' and '2024-05-09T23:59:59'\n",
      "trip_end_timestamp between '2024-05-10T00:00:00' and '2024-05-10T23:59:59'\n",
      "trip_end_timestamp between '2024-05-11T00:00:00' and '2024-05-11T23:59:59'\n",
      "trip_end_timestamp between '2024-05-12T00:00:00' and '2024-05-12T23:59:59'\n",
      "trip_end_timestamp between '2024-05-13T00:00:00' and '2024-05-13T23:59:59'\n",
      "trip_end_timestamp between '2024-05-14T00:00:00' and '2024-05-14T23:59:59'\n",
      "trip_end_timestamp between '2024-05-15T00:00:00' and '2024-05-15T23:59:59'\n",
      "trip_end_timestamp between '2024-05-16T00:00:00' and '2024-05-16T23:59:59'\n",
      "trip_end_timestamp between '2024-05-17T00:00:00' and '2024-05-17T23:59:59'\n",
      "trip_end_timestamp between '2024-05-18T00:00:00' and '2024-05-18T23:59:59'\n",
      "trip_end_timestamp between '2024-05-19T00:00:00' and '2024-05-19T23:59:59'\n",
      "trip_end_timestamp between '2024-05-20T00:00:00' and '2024-05-20T23:59:59'\n",
      "trip_end_timestamp between '2024-05-21T00:00:00' and '2024-05-21T23:59:59'\n",
      "trip_end_timestamp between '2024-05-22T00:00:00' and '2024-05-22T23:59:59'\n",
      "trip_end_timestamp between '2024-05-23T00:00:00' and '2024-05-23T23:59:59'\n",
      "trip_end_timestamp between '2024-05-24T00:00:00' and '2024-05-24T23:59:59'\n",
      "trip_end_timestamp between '2024-05-25T00:00:00' and '2024-05-25T23:59:59'\n",
      "trip_end_timestamp between '2024-05-26T00:00:00' and '2024-05-26T23:59:59'\n",
      "trip_end_timestamp between '2024-05-27T00:00:00' and '2024-05-27T23:59:59'\n",
      "trip_end_timestamp between '2024-05-28T00:00:00' and '2024-05-28T23:59:59'\n",
      "trip_end_timestamp between '2024-05-29T00:00:00' and '2024-05-29T23:59:59'\n",
      "trip_end_timestamp between '2024-05-30T00:00:00' and '2024-05-30T23:59:59'\n",
      "trip_end_timestamp between '2024-05-31T00:00:00' and '2024-05-31T23:59:59'\n",
      "trip_end_timestamp between '2024-06-01T00:00:00' and '2024-06-30T23:59:59'\n",
      "trip_end_timestamp between '2024-07-01T00:00:00' and '2024-07-31T23:59:59'\n",
      "trip_end_timestamp between '2024-08-01T00:00:00' and '2024-08-31T23:59:59'\n",
      "trip_end_timestamp between '2024-08-01T00:00:00' and '2024-08-01T23:59:59'\n",
      "trip_end_timestamp between '2024-08-02T00:00:00' and '2024-08-02T23:59:59'\n",
      "trip_end_timestamp between '2024-08-03T00:00:00' and '2024-08-03T23:59:59'\n",
      "trip_end_timestamp between '2024-08-04T00:00:00' and '2024-08-04T23:59:59'\n",
      "trip_end_timestamp between '2024-08-05T00:00:00' and '2024-08-05T23:59:59'\n",
      "trip_end_timestamp between '2024-08-06T00:00:00' and '2024-08-06T23:59:59'\n",
      "trip_end_timestamp between '2024-08-07T00:00:00' and '2024-08-07T23:59:59'\n",
      "trip_end_timestamp between '2024-08-08T00:00:00' and '2024-08-08T23:59:59'\n",
      "trip_end_timestamp between '2024-08-09T00:00:00' and '2024-08-09T23:59:59'\n",
      "trip_end_timestamp between '2024-08-10T00:00:00' and '2024-08-10T23:59:59'\n",
      "trip_end_timestamp between '2024-08-11T00:00:00' and '2024-08-11T23:59:59'\n",
      "trip_end_timestamp between '2024-08-12T00:00:00' and '2024-08-12T23:59:59'\n",
      "trip_end_timestamp between '2024-08-13T00:00:00' and '2024-08-13T23:59:59'\n",
      "trip_end_timestamp between '2024-08-14T00:00:00' and '2024-08-14T23:59:59'\n",
      "trip_end_timestamp between '2024-08-15T00:00:00' and '2024-08-15T23:59:59'\n",
      "trip_end_timestamp between '2024-08-16T00:00:00' and '2024-08-16T23:59:59'\n",
      "trip_end_timestamp between '2024-08-17T00:00:00' and '2024-08-17T23:59:59'\n",
      "trip_end_timestamp between '2024-08-18T00:00:00' and '2024-08-18T23:59:59'\n",
      "trip_end_timestamp between '2024-08-19T00:00:00' and '2024-08-19T23:59:59'\n",
      "trip_end_timestamp between '2024-08-20T00:00:00' and '2024-08-20T23:59:59'\n",
      "trip_end_timestamp between '2024-08-21T00:00:00' and '2024-08-21T23:59:59'\n",
      "trip_end_timestamp between '2024-08-22T00:00:00' and '2024-08-22T23:59:59'\n",
      "trip_end_timestamp between '2024-08-23T00:00:00' and '2024-08-23T23:59:59'\n",
      "trip_end_timestamp between '2024-08-24T00:00:00' and '2024-08-24T23:59:59'\n",
      "trip_end_timestamp between '2024-08-25T00:00:00' and '2024-08-25T23:59:59'\n",
      "trip_end_timestamp between '2024-08-26T00:00:00' and '2024-08-26T23:59:59'\n",
      "trip_end_timestamp between '2024-08-27T00:00:00' and '2024-08-27T23:59:59'\n",
      "trip_end_timestamp between '2024-08-28T00:00:00' and '2024-08-28T23:59:59'\n",
      "trip_end_timestamp between '2024-08-29T00:00:00' and '2024-08-29T23:59:59'\n",
      "trip_end_timestamp between '2024-08-30T00:00:00' and '2024-08-30T23:59:59'\n",
      "trip_end_timestamp between '2024-08-31T00:00:00' and '2024-08-31T23:59:59'\n"
     ]
    }
   ],
   "source": [
    "# Takes ~8m per month. Took overnight and half a day to get Jan - Aug\n",
    "uber_rides = soda_get_uber(\"2024-01-16\", \"2024-08-31\", \n",
    "                has_header=True,\n",
    "                select=\"\"\"date_trunc_ymd(trip_end_timestamp) as end_date, \n",
    "                          dropoff_census_tract,\n",
    "                          count(trip_id) as rides\"\"\",\n",
    "                group=\"end_date, dropoff_census_tract\")\n",
    "# uber_rides = []\n",
    "# from glob import glob\n",
    "# for f in glob(\"uber-2024-*.csv\"):\n",
    "#     uber_rides.append(pd.read_csv(f))\n",
    "# uber_rides = pd.concat(uber_rides, ignore_index=True)\n",
    "# uber_rides['start_date'] = pd.to_datetime(uber_rides['start_date'], 'coerce')\n",
    "# uber_rides = uber_rides.groupby([\"start_date\",\"pickup_census_tract\"], as_index=False)['rides'].sum()\n",
    "# uber_rides = uber_rides.assign(UCMP16 = uber_rides['pickup_census_tract'].map(tract_points.set_index('geoid10')['UCMP16']))\n",
    "# uber_rides['UCMP16'] = uber_rides['UCMP16'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'start_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Dev/DNCTransit/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m uber_weekly \u001b[38;5;241m=\u001b[39m \u001b[43muber_rides\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_weekly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUCMP16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrides\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mline(uber_weekly, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrides\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUCMP16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m fig\u001b[38;5;241m.\u001b[39madd_vline(x\u001b[38;5;241m=\u001b[39mDNC_ISO_WEEK, line_dash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdash\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/DNCTransit/.venv/lib/python3.12/site-packages/pandas/core/generic.py:6231\u001b[0m, in \u001b[0;36mNDFrame.pipe\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m   6230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m common\u001b[38;5;241m.\u001b[39mpipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 6231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/DNCTransit/.venv/lib/python3.12/site-packages/pandas/core/common.py:502\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m(obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m, in \u001b[0;36magg_weekly\u001b[0;34m(df, datecol, groupcols, agg_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_weekly\u001b[39m(df, datecol, groupcols, agg_func):\n\u001b[0;32m----> 2\u001b[0m     iso \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatecol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39misocalendar()\n\u001b[1;32m      3\u001b[0m     weeks \u001b[38;5;241m=\u001b[39m iso\u001b[38;5;241m.\u001b[39mweek\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{datecol: iso\u001b[38;5;241m.\u001b[39myear\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m weeks})\n",
      "File \u001b[0;32m~/Dev/DNCTransit/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Dev/DNCTransit/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_date'"
     ]
    }
   ],
   "source": [
    "uber_weekly = uber_rides.pipe(agg_weekly, 'start_date', ['UCMP16'], {'rides':'sum'})\n",
    "fig = px.line(uber_weekly, x='start_date', y='rides', color='UCMP16')\n",
    "fig.add_vline(x=DNC_ISO_WEEK, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis=dict(type='category'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Post Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate:\n",
    "\n",
    "$$ rides_t = \\beta_0 + \\beta_1 \\text{DNC}_t$$\n",
    "\n",
    "with two weeks of data, where $\\text{DNC}_t=1$ during the DNC week and 0 otherwise.\n",
    "Note there is no error term because we have pooled over all units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_start_iso = (datetime.fromisoformat(DNC_START_ISO) - timedelta(days=7)).isoformat()\n",
    "control_end_iso = (datetime.fromisoformat(DNC_END_ISO) - timedelta(days=7)).isoformat()\n",
    "\n",
    "where_date = f\"\"\"(service_date between '{DNC_START_ISO}' and '{DNC_END_ISO}') OR\n",
    "                (service_date between '{control_start_iso}' and '{control_end_iso}')\"\"\"\n",
    "rides = get_rides_panel(control_start_iso, control_end_iso, DNC_START_ISO, DNC_END_ISO) \\\n",
    "        .assign(DNC = lambda x: (x['date'] >= DNC_START_ISO) & (x['date'] <= DNC_END_ISO))\n",
    "\n",
    "model_data = rides.groupby('DNC')['rides'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the estimates by hand because we literallly only have two observations!\n",
    "naive_prepost = {\n",
    "    \"beta_0\" : model_data[False],\n",
    "    \"beta_1\" : model_data[True] - model_data[False]\n",
    "}\n",
    "naive_prepost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that there are alot more rides during the DNC week than the prior week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Post OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate:\n",
    "\n",
    "$$ rides_{it} = \\beta_0 + \\beta_1 \\text{DNC}_t + u_i$$\n",
    "\n",
    "with two weeks of data, where $\\text{DNC}_t=1$ during the DNC week and 0 otherwise.\n",
    "Note now we have an error term because we are allowing variation across units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to follow Woldridge 13.2, we are pooling these daily observations\n",
    "#       into just two time periods: before and during. We won't sweat the \n",
    "#       implications of this choice on the naive model.\n",
    "model_data = rides.groupby(['tid','DNC'], as_index=False)['rides'].sum()\n",
    "naive_pre_post = sm.OLS.from_formula(\"rides ~ DNC\", model_data).fit()\n",
    "print(naive_pre_post.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the DNC week does not significantly affect average rides systemwide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But the box plot confirms that the unit-level differences are not significant.\n",
    "px.box(model_data, x='DNC', y='rides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Section During DNC Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate:\n",
    "\n",
    "$$ rides_i = \\beta_0 + \\beta_2 \\text{UC}_i + u_i $$\n",
    "\n",
    "during the DNC week, where $\\text{UC}_i$=1 if the unit is within the United Center security perimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For starters lets look at the maximal version of this model:\n",
    "rides['UCMP16'] = (rides['uc_1600'] > 0) | (rides['mp_1600'] > 0)\n",
    "model_data = rides.groupby(['tid','DNC','UCMP16'], as_index=False)['rides'].sum()\n",
    "naive_xs = sm.OLS.from_formula(\"rides ~ UCMP16\", model_data, subset=model_data['DNC']).fit()\n",
    "print(naive_xs.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that stations near the DNC on average may have had more rides than other stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The box plot confirms that the means (at least) are marginally different\n",
    "px.box(model_data[model_data['DNC']], x='UCMP16', y='rides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catchements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the sensitivity of perimeter radii in the context of the cross sectional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides['UC4'] = (rides['uc_400'] > 0)\n",
    "rides['UC8'] = (rides['uc_800'] > 0)\n",
    "rides['UC16'] = (rides['uc_1600'] > 0)\n",
    "rides['MP4'] = (rides['mp_400'] > 0)\n",
    "rides['MP8'] = (rides['mp_800'] > 0)\n",
    "rides['MP16'] = (rides['mp_1600'] > 0)\n",
    "rides['UCMP4'] = (rides['uc_400'] > 0) | (rides['mp_400'] > 0)\n",
    "rides['UCMP8'] = (rides['uc_800'] > 0) | (rides['mp_800'] > 0)\n",
    "rides['UCMP16'] = (rides['uc_1600'] > 0) | (rides['mp_1600'] > 0)\n",
    "\n",
    "# Note: to follow the textbook examples, we are pooling these daily observations\n",
    "#       into just two time periods: before and during. We won't sweat the \n",
    "#       implications of this choice on the naive model.\n",
    "uc_cols = rides.filter(regex=\"UC|MP\").columns\n",
    "model_data = rides[rides['DNC']].groupby(['tid'] + list(uc_cols), as_index=False)['rides'].sum()\n",
    "naive_uc4 = sm.OLS.from_formula(\"rides ~ UC4\", model_data).fit()\n",
    "naive_uc8 = sm.OLS.from_formula(\"rides ~ UC8\", model_data).fit()\n",
    "naive_uc16 = sm.OLS.from_formula(\"rides ~ UC16\", model_data).fit()\n",
    "naive_mp4 = sm.OLS.from_formula(\"rides ~ MP4\", model_data).fit()\n",
    "naive_mp8 = sm.OLS.from_formula(\"rides ~ MP8\", model_data).fit()\n",
    "naive_mp16 = sm.OLS.from_formula(\"rides ~ MP16\", model_data).fit()\n",
    "naive_ucmp4 = sm.OLS.from_formula(\"rides ~ UCMP4\", model_data).fit()\n",
    "naive_ucmp8 = sm.OLS.from_formula(\"rides ~ UCMP8\", model_data).fit()\n",
    "naive_ucmp16 = sm.OLS.from_formula(\"rides ~ UCMP16\", model_data).fit()\n",
    "\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "summary_col([naive_uc4,naive_uc8,naive_uc16,\n",
    "             naive_mp4,naive_mp8,naive_mp16,\n",
    "             naive_ucmp4,naive_ucmp8,naive_ucmp16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the stations near these places have more rides on average,\n",
    "and that the stations sort of near these places have average or below average rides.\n",
    "Moreover, the UC-area stations have more ridership than MP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you see below, there's barely any stops within the 400m buffer.\n",
    "pd.Series(rides.filter(regex=r\"uc|mp\").sum(), name=\"treat size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of these models, we will use the 1600m buffer as our covariate, \n",
    "because even though the effect size\n",
    "is most attenuated there, this gives us the largest treatment size. So this is \n",
    "actually the most conservative option and reduces variance in our errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a Choose Chicago [report](https://cdn.choosechicago.com/uploads/2024/10/TE-DNC-Impact.pdf), the DNC attracted 50,000 visitors.\n",
    "\n",
    "Given our sample size of ?? time periods and stations, desired power, and desired alpha,\n",
    "what effect sizes can we predict?\n",
    "\n",
    "Following [Bloom (1995)](https://journals.sagepub.com/doi/epdf/10.1177/0193841X9501900504):\n",
    "\n",
    "the minimum detectable effect size is simply computed by comparing the one-sided\n",
    "z score to reject the null hypothesis vs the one-sided z score to accept the alternate\n",
    "hypothesis. i.e. for $\\alpha=.05$ and $\\beta=.8$ we have $MDE = 2.49\\sigma_c$\n",
    "whereas for $\\alpha=.1$ and $\\beta=.8$ we have $MDE = 2.12\\sigma_c$ where $\\sigma_c$\n",
    "is the standard error of the estimate.\n",
    "\n",
    "One can easily extend this analysis to two-sided cases. Bloom uses one-sided for its\n",
    "simplicity in testing whether the *intended* effect happened or not. One-sided\n",
    "tests have greater statistical power (produce smaller MDE's) than two-sided.\n",
    "But in practice researchers do use the two-sided test because they can't theoretically rule out\n",
    "the possibility of observing an unintended effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bloom presents this formula for the standard error, noted as sigma_c\n",
    "# sigma2 = rides['rides'].var()\n",
    "# R2 = naive_pre_post.rsquared\n",
    "# T = rides['DNC'].mean()\n",
    "# n = len(rides)\n",
    "# sigma_c = np.sqrt(sigma2 * (1 - R2) / (T * (1 - T) * n))\n",
    "zcritical = pd.Series({\".1\": 2.12, \".05\": 2.49},name=\"zcritical\")\n",
    "# zcritical * sigma_c\n",
    "\n",
    "# Conveniently, sigma_c == statsmodels.model.bse\n",
    "def mde(model):\n",
    "    return pd.concat(\n",
    "        [pd.DataFrame(np.outer(model.bse, zcritical),\n",
    "             columns=\"MDE at \" + zcritical.index, \n",
    "             index=model.bse.index),\n",
    "        pd.Series(model.params, name='estimated')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde(naive_pre_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows we are severely underpowered to reject the null hypothesis if the\n",
    "true effect sizes are indeed equal to these estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde(naive_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows we almost have enough power if the true effect size is indeed as big as estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Panel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "Maybe refactor using the Linear Models package as in :\n",
    "https://medium.com/data-science-at-microsoft/reducing-omitted-variable-bias-with-fixed-effects-regression-models-8a132a8f2c44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these models Woldridge provides examples that only compare two time periods,\n",
    "usually two (not necessarily consecutive) years. Somehow he doesn't say anything\n",
    "about parallel trends in DiD, but it seems possible that it's common to use a two-period design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff in Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "The naive cross sectional model assumes all differences between the two groups\n",
    "are due to the treatment, but maybe those differences already existed.\n",
    "\n",
    "The naive panel model assumes that all differences between the two time periods\n",
    "are due to the treatment, without actually specifying who was assigned to treatment.\n",
    "Units in the control group may experience changes in ridership unrelated to the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple difference-in-difference model combines the naive models with two weeks of data:\n",
    "\n",
    "$$ rides_{it} = \\beta_0 + \\beta_1 \\text{DNC}_t + \\beta_2 \\text{UC}_i + \\beta_3 \\text{DNC}_t \\text{UC}_i  + u_{it} $$\n",
    "\n",
    "where $\\text{DNC}_t=1$ during the DNC week and 0 otherwise, and $\\text{UC}_i=1$ if the station is inside the security perimeter and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you see below, there's barely any stops within the 400m buffer, so \n",
    "# this would be a terrible treatment sample.\n",
    "pd.Series(rides.filter(regex=r\"uc|mp\").sum(), name=\"treat size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to follow Woldridge 13.2, we pool the daily observations into\n",
    "#       two time periods. \n",
    "model_data = rides.groupby(['DNC','UCMP16','tid'],as_index=False)['rides'].sum()\n",
    "did_model = sm.OLS.from_formula(\"rides ~ UCMP16 * DNC\", model_data).fit()\n",
    "print(did_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the United Center and McCormick Place generally have more rides,\n",
    "but that the DNC week didn't have significantly more rides, and UC and MP didn't\n",
    "have significantly more rides during the DNC week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Omitted Time and Space Varying Coefs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the incinerator example of Woldridge 13.2, the kinds of houses up for sale near the incinerator\n",
    "may be different in both years. The follow-up question is whether those changes\n",
    "are related to the incinerator.\n",
    "\n",
    "In our example, the kinds of riders, quality of service, rider destinations\n",
    "might be different near the DNC than not. For example, a concurrent concert or sporting event,\n",
    "a public safety issue on the trains, track maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Trends Assumption**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_start = datetime.strptime(DNC_START, YMD) - timedelta(days=31)\n",
    "trend_start_iso = trend_start.isoformat()\n",
    "trend_end = datetime.strptime(DNC_END, YMD).replace(hour=23,minute=59,second=59) - timedelta(days=7)\n",
    "trend_end_iso = trend_end.isoformat()\n",
    "\n",
    "rides_trend = get_rides_panel(trend_start_iso, trend_end_iso, DNC_START_ISO, DNC_END_ISO)\n",
    "rides_trend['UCMP16'] = (rides_trend['uc_1600'] > 0) | (rides_trend['mp_1600'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's hard to see whether there are parallel trends here... too many data points.\n",
    "fig = px.line(rides_trend, x='date', y='rides', color='UCMP16')\n",
    "fig.add_vline(x=trend_end.strftime(YMD), line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_vline(x=DNC_START, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_trend_agg = rides_trend.groupby(['date','UCMP16'],as_index=False)['rides'].agg(['mean','min','max'])\n",
    "rides_trend_agg = rides_trend_agg.melt(id_vars=['date','UCMP16'], value_vars=['mean','min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can sort of see there are parallel trends within each quantile.\n",
    "# TODO! But I'm not sure if I'm accounting for quantile very well. Might need matched design.\n",
    "fig = px.line(rides_trend_agg, x='date', y='value', color='UCMP16', line_dash='variable')\n",
    "fig.add_vline(x=trend_end.strftime(YMD), line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_vline(x=DNC_START, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stable Unit Treatment Value Assumption (SUTVA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's definitely a possibility of SUTVA violation considering how ad-hoc\n",
    "the spatial treatment areas are defined. If spillover is happening, then we\n",
    "ought to see a positive DNC coef but an attenuated DNC*UC coef."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exog of Treatment**\n",
    "\n",
    "This is definitely a possibility. The DiD doesn't fully account for this because\n",
    "even though it separates out UC vs Non-UC at baseline, we actually expect UC to\n",
    "be *more* able to accomodate surges in transit compared to other areas. So this\n",
    "sounds like a threat to external validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate:\n",
    "\n",
    "$$\n",
    "\\text{rides}_{it} = \\beta_0 + \\beta_1 \\text{DNC}_t + \\beta_2 \\text{UC}_i + \\gamma_i +  u_{it}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = rides.groupby(['DNC','UCMP16','tid'],as_index=False)['rides'].sum()\n",
    "fe_model = sm.OLS.from_formula(\"rides ~ DNC + UCMP16 + C(tid)\", model_data).fit()\n",
    "print(fe_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col([naive_pre_post, naive_xs, did_model, fe_model], \n",
    "            model_names=['Naive I','Naive II','DiD','FE'],\n",
    "            stars=True,\n",
    "            regressor_order=['Intercept',\"DNC[T.True]\",\"UCMP16[T.True]\"], drop_omitted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = \" = \".join(fe_model.params.index[fe_model.params.index.str.startswith(\"C(tid)\")]) + \" = 0\"\n",
    "fe_model.f_test(constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The this shows the R2 suddenly becomes extremely tight, due to finally controlling for station/line variance.\n",
    "\n",
    "The DNC also finally becomes significant, but the UCMP (which was always sig) interestingly switches sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\gamma_i$ uncorrelated with $x_{it}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamma's aren't the same as controlling for time-constant variables. They\n",
    "actually represent unobserved time-constant (i.e. fixed) effects. \n",
    "\n",
    "Are these unmeasured station characteristics correlated with UC? Likely yes\n",
    "because UC is a strong transit attraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "We want to eliminate the need to estimate these unobserved idiosyncratic time-invariant effects, $\\gamma_i$.\n",
    "Even though we can estimate their intercepts, they're not actually \"observed\".\n",
    "Typically we consider $\\gamma_i$ as a decomposition of the error term  $u_{it} = \\gamma_i + e_{it}$,\n",
    "therefore if $\\gamma_i$ are correlated with X, this breaks $E(cov(X,u))=0$ assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{rides}_{t+1} - \\text{rides}_t &= \\\\\n",
    "& \\beta_0 + \\beta_1 (\\text{DNC}_1=1) + \\beta_2 \\text{UC}_i + \\gamma_i +  u_{i1} \\\\\n",
    "-& \\beta_0 + \\beta_1 (\\text{DNC}_0=0) + \\beta_2 \\text{UC}_i + \\gamma_i +  u_{i0} \\\\\n",
    "\\Delta \\text{rides}_i &=  \\beta_1 + \\Delta u_i\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This drops away the $\\gamma_i$'s, allowing them to be correlated with X, but \n",
    "without biasing the model specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Woldridge p. 467, $\\beta_2$ is the same as the DiD estimate if \n",
    "the program participation only occurs in the second period. This makes sense because\n",
    "they're writing it as $\\beta \\text{prog}_{it}$ which is equivalent to \n",
    "AND-ing the treatment assignment and treatment period as in  \n",
    "$\\beta (\\text{DNC}_t \\& \\text{UC}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this method will not work because we have not used X's that vary over time.\n",
    "We cannot separate the effect of UC from the other time-invariant unit effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specifying the model anyway, but note that UCMP is basically part of gamma\n",
    "# # because it isn't time-varying. In a sense it's perfectly co-linear. So we can't\n",
    "# # use regression to separate these variables.\n",
    "# model_data = rides.groupby(['tid','UCMP16','DNC'], sort=True, as_index=False)['rides'].sum()\n",
    "# is_full_panel = model_data.tid.value_counts() == 2\n",
    "# model_data = model_data[model_data['tid'].isin(is_full_panel[is_full_panel].index)]\n",
    "# assert(all(model_data['tid'].value_counts() == 2))\n",
    "# model_data = model_data[model_data['DNC']].set_index('tid')[['rides','UCMP16']]*1.0 \\\n",
    "#             - model_data[~model_data['DNC']].set_index('tid')[['rides','UCMP16']]*1.0\n",
    "# model_data = model_data.reset_index()\n",
    "# assert(all(model_data['UCMP16'] == 0))\n",
    "# fd_model = model_data['rides'].mean()\n",
    "# fd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just curious how this is distributed.\n",
    "# px.box(model_data['rides'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately Woldridge p. 461 says that the assumptions rule out allowing\n",
    "any X's to be a lagged Y ie $x_{it} = y_{i,t-1}$ because that would allow $\\Delta u_i$ to correlate\n",
    "with $\\Delta X_i$. \n",
    "\n",
    "Also $\\Delta X_i$ kind of gets rid of per-unit levels of explanatory variables\n",
    "so it reduces variation in X if the timelike variation isn't as large. \n",
    "(We actually saw above that the timelike variation is zero, and couldnt run the model).\n",
    "\n",
    "For example: $\\text{income}_{it} \\sim \\text{education}_{it}$ yields $\\Delta \\text{income}_{i} \\sim \\Delta \\text{education}_i$\n",
    "but even though the panel might have a lot of variation education, it might not have \n",
    "lots of variation in *changes* in education (like once you're an adult your education doesn't change as much).\n",
    "\n",
    "Woldridge (p. 467) notes that the differenced estimator is the two-period panel version of DiD. But it has the advantage of being able to control for time invariant unit effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time De-Meaned FE\n",
    "\n",
    "(aka Fixed Effects Transformation aka Within Transformation aka Within Estimator)\n",
    "\n",
    "(It seems like this is what they mean whenever they talk about FE models later on.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define \n",
    "\n",
    "$$\\ddot y_{it} = y_{it} - \\bar y_i = y_{it} - T^{-1}\\sum_{t=1}^T{y_{it}} $$\n",
    "\n",
    "and so on for X and u.\n",
    "\n",
    "Then subtract $\\ddot y_{it}$ from the fixed-effects estimator, giving:\n",
    "\n",
    "$$\\ddot{\\text{rides}_{it}} = \\beta_1 \\ddot{\\text{DNC}_t} + \\ddot{u_{it}}$$\n",
    "\n",
    "where the intercept, fixed effects, and all time-invariant X drop out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the same as the FD model, so we can't use it for the same reasons.\n",
    "(In the T=2 case they are actually identical.)\n",
    "\n",
    "However, Woldridge p.491 says the bias in FE due to $\\text{Cov}(x,u)$ (e.g. \n",
    "due to including lagged dependent variables) tends to zero at a rate of 1/T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally Woldridge p.490 recommend the FE model when $u_{it}$ are serially uncorrelated,\n",
    "and FD when they are correlated or when y is a root process or integrated time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacted FD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact the time-invariant X with the year dummies and estimate:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{rides}_{t+1} - \\text{rides}_t &= \\\\\n",
    "& \\beta_0 + \\beta_1 (\\text{DNC}_1=1) + \\beta_2 \\text{UC}_i + \\beta_3 (\\text{DNC}_1=1)*\\text{UC}_i + \\gamma_i +  u_{i1} \\\\\n",
    "-& \\beta_0 + \\beta_1 (\\text{DNC}_0=0) + \\beta_2 \\text{UC}_i + \\beta_3 (\\text{DNC}_0=0)*\\text{UC}_i + \\gamma_i +  u_{i0} \\\\\n",
    "\\Delta \\text{rides}_i &=  \\beta_1 + \\beta_3\\text{UC}_i + \\Delta u_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Interpreting $\\beta_3$ as the change in the return on UC over time. But in this model\n",
    "we can't estimate the baseline return on UC therefore we can't estimate the actual\n",
    "return on UC at either period. Still this seems promising since it sounds like\n",
    "DiD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Compared to the Fixed Effects model, where we explicitly estimate an idiosyncratic\n",
    "per-unit coefficient, the mixed linear effects model's random effects account for\n",
    "per-unit variation without wasting as many degrees of freedom.\n",
    "\n",
    "However it requires assuming the fixed effect $\\gamma$'s are uncorrelated with\n",
    "X for all time periods:\n",
    "\n",
    "$$\\text{Cov}(x_{itj}, \\gamma_i) = 0 \\text{ , } \\forall t,j$$\n",
    "\n",
    "But it allows including time-invariant X's!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In this block we use Woldridge's notation where $\\gamma_i = a_i$)\n",
    "\n",
    "We estimate a transformation factor as:\n",
    "\n",
    "$$\\theta = 1 - [\\sigma^2_u/(\\sigma^2_u + T\\sigma^2_a)]^{1/2}$$\n",
    "\n",
    "where $\\sigma^2_a = \\text{Var}(a_i)$ and where $\\sigma^2_u = \\text{Var}(u_{it})$\n",
    "and since u isn't known, there's a couple ways to estimate $\\hat \\theta$ which the computer handles.\n",
    "\n",
    "then we transform the pooled OLS equation as:\n",
    "\n",
    "$$\n",
    "y_{it} - \\theta \\bar y_i = \\beta_0(1-\\theta) + \\beta_1(x_{it} - \\theta \\bar x_i) + (a_i + u_{it} - \\theta (\\bar a_i + \\bar u_{it}))\n",
    "$$\n",
    "\n",
    "but really the computer does the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because we can't assume the fixed unit effects are uncorrelated with X.\n",
    "# Note: Now we don't have to pool dates because we are actually trying to handle within-unit variance.\n",
    "# mlm_model = sm.MixedLM.from_formula(\"rides ~ UCMP16 * DNC\", \n",
    "#                                     groups=\"tid\",\n",
    "#                                     data=rides).fit()\n",
    "# print(mlm_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Random Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Allows $\\gamma_i$ to be correlated with X, therefore allows X's that are time-invarient.\n",
    "However the time-invariant X's are no longer causal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICBST (see Woldridge 14.3) if you add a term which is the mean of x, \n",
    "you can decompose the fixed effects terms as\n",
    "\n",
    "$$\\gamma_i = \\alpha + \\nu \\hat x_i + r_i$$\n",
    "\n",
    "it basically\n",
    "removes the component of $\\gamma$ that was correlated with X. Therefore we estimate:\n",
    "\n",
    "$$\n",
    "y_{it} = \\alpha + \\beta x_{it} + \\delta \\hat x_i + r_i + u_{it}\n",
    "$$\n",
    " \n",
    "or\n",
    "\n",
    "$$\n",
    "y_{it} = \\alpha + \\beta x_{it} + \\delta \\hat x_i + \\epsilon z_i + r_i + u_{it}\n",
    "$$\n",
    "\n",
    "where $r_i$ is the new random effect term and $z_i$ are time-invariant x's.\n",
    "\n",
    "ICBST the $\\beta$'s of the time-varying X's are equivalent to the FE estimators.\n",
    "\n",
    "Note: there is no reason to include time-dummies in this formulation\n",
    "(if the panel is balanced) because the time-average of a time dummy is just a constant 1/T.\n",
    "Whereas if the panel is unbalanced I guess the time dummies are weighted by sample count\n",
    "so they actually do change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO! How to do this in statsmodels??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Time Panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can swap the time dimension for a hierarchical unit dimension ie. siblings within families, \n",
    "or train stations within train lines or maybe train stations within neighborhoods.\n",
    "\n",
    "We'd have to get rid of busses, but this could be an interesting way to use FE or FD.\n",
    "\n",
    "For example, given t=DNC, and j is community or line:\n",
    "$$\n",
    "\\text{rides}_{ij} = \\beta_0 + \\beta_1 \\text{UC}_{ij}  + \\gamma_j +  u_{ij}\n",
    "$$\n",
    "\n",
    "then we'd have to only assume that $\\gamma_j$ is uncorrelated with UC, which I probably don't belive.\n",
    "Woldridge says the order or comparison of i's across j aren't necessarily meaningful,\n",
    "ie if the i's are randomly labeled, then it just forces an intercept to zero, whereas\n",
    "othewise it means the earlier i's have systematically higher rides in the FD approach.\n",
    "\n",
    "Note that different cluster sizes (j's) turns the panel into unbalanced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it is worth log-transforming the rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = rides.groupby('tid',as_index=False)['rides'].sum() \\\n",
    "    .assign(logrides=lambda x: np.log(x['rides']))\n",
    "px.histogram(plot_data, x='logrides')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transit modes have clearly different means but similar variance. \n",
    "We'll include it by default because it can't hurt, but I wish it explained more variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(rides, x='transit', y='rides', title=\"Daily Rides per Station / Bus Line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly estimate just this variable.\n",
    "transit_model = sm.OLS.from_formula(\"rides ~ transit\", rides).fit()\n",
    "print(transit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Like the catchement covariates, I'm going to model this as binary\n",
    "#       which we interpret as if the model serves the station.\n",
    "rides['AIR'] = rides['airport'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(rides, x='AIR', y='rides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distributions grouped by transit type interacted with airport.\n",
    "# Notice the bus lines serving Midway have slightly less ridership on average,\n",
    "# while the OHare and Midway stops have mostly out-of-distribution ridership\n",
    "# compared to the rest of the train stations. \n",
    "# Indicating we should definitely include these as covariates, interacted together.\n",
    "px.box(rides, x='transit', color='AIR', y='rides', title=\"Daily Rides per Station / Bus Line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_air_model = sm.OLS.from_formula(\"rides ~ transit * AIR\", rides).fit()\n",
    "print(transit_air_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "This is to better control for ridership, which is not equally distributed across the city.\n",
    "For instance, I'm guessing the different signs of the train vs bus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's standardize lat/long because they are measured in millions of feet\n",
    "# So doing so helps keep the relative scale of X vs Y in check. \n",
    "rides['lat_c'] = (rides['lat'] - rides['lat'].mean()) / rides['lat'].std()\n",
    "rides['long_c'] = (rides['long'] - rides['long'].mean()) / rides['long'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There doesn't seem to be a really strong simple relationship\n",
    "plot_data = rides.groupby(['tid','lat_c','long_c'],as_index=False)['rides'].sum().assign(\n",
    "        latlon = lambda x: x['lat_c'] * x['long_c'],\n",
    "        lat2 = lambda x: x['lat_c'] * x['lat_c'],\n",
    "        lon2 = lambda x: x['long_c'] * x['long_c'],\n",
    "        logrides = lambda x: np.log(x['rides']),\n",
    ").melt(['tid','rides','logrides'])\n",
    "px.scatter(plot_data, x='value', y='logrides', facet_col='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlong_formula = \"(lat_c + long_c)**2 + I(lat_c**2) + I(long_c**2)\"\n",
    "latlong_model = sm.OLS.from_formula(f\"rides ~ {latlong_formula}\", rides).fit()\n",
    "print(latlong_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly this doesn't help our R2 as much as I expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytype_model = sm.OLS.from_formula(\"rides ~ daytype\", rides).fit()\n",
    "print(daytype_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.OLS.from_formula(\"rides ~ C(date)\", rides).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant but also doesn't explain a lot of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs_cols = [\"daytype\", 'transit','AIR','lat_c','long_c']\n",
    "covs_formula = f\"daytype + transit * AIR + {latlong_formula}\"\n",
    "covs_model = sm.OLS.from_formula(f\"rides ~ {covs_formula}\", rides).fit()\n",
    "print(covs_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_rides = rides['tid'].isin(rides.groupby('tid',as_index=False)['rides'].sum().query('rides>0')['tid'])\n",
    "log_covs_model = sm.OLS.from_formula(f\"np.log(rides) ~ {covs_formula}\", \n",
    "                                     rides, subset=has_rides).fit()\n",
    "print(log_covs_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo DiD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = rides.groupby(['DNC','UCMP16','tid'] + covs_cols,as_index=False)['rides'].sum()\n",
    "did_covs_model = sm.OLS.from_formula(f\"rides ~ UCMP16 * DNC + {covs_formula}\", model_data).fit()\n",
    "print(did_covs_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = rides.groupby(['DNC','UCMP16','tid'] + covs_cols,as_index=False)['rides'].sum()\n",
    "fe_covs_model = sm.OLS.from_formula(f\"rides ~ UCMP16 * DNC + {covs_formula} + C(tid)\", model_data).fit()\n",
    "print(fe_covs_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Now we don't have to pool dates because we are actually trying to handle within-unit variance.\n",
    "model_data = rides.groupby(['DNC','UCMP16','tid'] + covs_cols,as_index=False)['rides'].sum()\n",
    "mlm_covs_model = sm.MixedLM.from_formula(f\"rides ~ UCMP16 * DNC + {covs_formula}\", \n",
    "                                    groups=\"tid\",\n",
    "                                    data=model_data).fit()\n",
    "print(mlm_covs_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = rides.drop_duplicates(['tid','transit','UCMP16'])\n",
    "px.imshow(pd.crosstab(plot_data['transit'], plot_data['UCMP16']), text_auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = rides.drop_duplicates(['tid','AIR','UCMP16'])\n",
    "px.imshow(pd.crosstab(plot_data['AIR'], plot_data['UCMP16']), text_auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = rides.drop_duplicates(['tid','lat_c','UCMP16'])\n",
    "px.box(plot_data, y='lat_c',x='UCMP16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = rides.drop_duplicates(['tid','long_c','UCMP16'])\n",
    "px.box(plot_data, y='long_c',x='UCMP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model = sm.Logit.from_formula(\"I((UCMP16|AIR)*1.0) ~ transit + lat_c + long_c\", rides).fit()\n",
    "rides['prop_logit'] = pr_model.predict(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.violin(rides, x='UCMP16', y='prop_logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "Find better tutorial on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "Decide what time periods you want to include. This will significantly decrease\n",
    "query times for bike and rail tables.\n",
    "\n",
    "TODO!\n",
    "\n",
    "Also decide the time granularity. Do you need it to be daily? Weekly may also make sense.\n",
    "That may help smooth out the variation meaning you can use a simpler model. \n",
    "Otherwise a linear model will lose a lot of explanatory power comparing weekdays to weekends.\n",
    "But maybe just add a weekday and also quarter or month fixed effect and plot the residuals of all the predicted vs real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = soda_get_all(TOTAL_RIDERSHIP_TABLE, \n",
    "             select=\"service_date AS date, total_rides AS rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['is_dnc'] = (DNC_START_ISO <= ts['date']) & (ts['date'] <= DNC_END_ISO)\n",
    "ts = pd.concat([ts, ts.date.dt.isocalendar()], axis=1)\n",
    "ts['yearweek'] = ts['year'].astype(str).str.cat(ts['week'].astype(str).str.pad(2,'left','0'),sep='-')\n",
    "ts = ts.groupby('date',as_index=False).last()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The process looks stationary but after pandemic there is a growth factor.\n",
    "# I think we should take only post pandemic data because otherwise we have to\n",
    "# add a post-pandemic interaction to all the regression terms.\n",
    "ts.plot(x='date',y='rides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-plotting at weekly to reduce high-frequency variation.\n",
    "# (Omitted: validated that the periodic drops are due to \n",
    "# the christmas holiday season. the iso transformation creates stable number of\n",
    "# rows per week and per year.)\n",
    "ts.groupby('yearweek')['rides'].sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = ts[ts.date > datetime.strptime(COVID['PHASE_5'], YMD)].groupby('yearweek',as_index=False)['rides'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# This is pretty bad. It may or may not be seasonal. \n",
    "# If it is seasonal, this is the wrong way to estimate it.\n",
    "stl = STL(model_data['rides'], period=52)\n",
    "res = stl.fit()\n",
    "fig = res.plot()\n",
    "\n",
    "\n",
    "# # Same with this.\n",
    "# res = seasonal_decompose(model_data['rides'], model = \"additive\",period = 52)\n",
    "\n",
    "# fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,8))\n",
    "# res.trend.plot(ax=ax1,ylabel = \"trend\")\n",
    "# res.resid.plot(ax=ax2,ylabel = \"seasoanlity\")\n",
    "# res.seasonal.plot(ax=ax3,ylabel = \"residual\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |L1| < 1 implies that the process is integrated of order zero, I(0)\n",
    "# Differencing will allow us to use OLS, but it's unclear if we need to difference or not,\n",
    "# because they also say you need to difference if L1 is close to 1 like > .9 or > .8\n",
    "# But they later say to obtain the first order autocrrelation AFTER de-trending.\n",
    "ar1 = AutoReg(model_data['rides'], 1).fit()\n",
    "print(ar1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([model_data, pd.Series(ar1.predict(),name='AR1')], axis=1) \\\n",
    "    .melt(id_vars='yearweek', value_vars=['rides','AR1'])\n",
    "fig = px.line(plot_data, x='yearweek', y='value', color='variable', title='Observed vs Predicted')\n",
    "fig.update_layout(xaxis=dict(type='category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AR1 model is literally the other series shifted and slightly attenuated\n",
    "# So our eye is going to make the fit look better than it really is because our\n",
    "# eye matches the diagnoal/horizontal distance, not the vertical distance.\n",
    "# Instead plot the residuals as below ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AR1 model actually seems to do very well in explaining the variance.\n",
    "# But you can see there is a ~52 week periodicity in the residuals.\n",
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = ar1.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows why them model is well-fit by AR-1. \n",
    "# The first order difference is mean-zero, finite variance.\n",
    "model_data_delta = model_data.assign(\n",
    "    delta = model_data['rides'].diff(),\n",
    "    pct_delta = lambda x: x['delta']/x['rides'])\n",
    "\n",
    "# Drop first row, which has nothign to difference against.\n",
    "model_data_delta = model_data_delta.iloc[1:].reset_index(drop=True)\n",
    "print(f\"mean: {model_data_delta['pct_delta'].mean()}, var: {model_data_delta['pct_delta'].var()}\")\n",
    "fig = px.line(model_data_delta, x='yearweek', y='pct_delta', title='first difference of Y')\n",
    "fig.update_layout(xaxis=dict(type='category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying to wrap my head around how good this model is statistically. Recall when we call predict we use:\n",
    "\n",
    "$$Y_t = \\alpha + \\rho_1 Y_{t-1} + e_t$$\n",
    "\n",
    "When we call predict on the training set, it uses the actual observed values each time\n",
    "and then applies some noise, right? Of course the fit is going to be good, because\n",
    "it only applies a small deviation to the previous week's value. And since the \n",
    "dataset itself doesn't have large fluctuations independent of the previous value,\n",
    "this model suits well. This quickly becomes terrible for prediction though, \n",
    "because obviously the true process is not that simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(model_data) * .5)\n",
    "train, test = model_data[:train_size]['rides'], model_data[train_size:]['rides']\n",
    "\n",
    "ar1_train = sm.tsa.AutoReg(train, lags=1)\n",
    "ar1_train_fit = ar1_train.fit()\n",
    "\n",
    "ar1_pred_train = ar1_train_fit.predict(0, len(train)-1)\n",
    "ar1_pred_test = ar1_train_fit.predict(start=len(train), end=len(train)+len(test)-1)\n",
    "\n",
    "plot_data = pd.concat([model_data, pd.Series(pd.concat([ar1_pred_train, ar1_pred_test]),name='pred')], axis=1)\n",
    "\n",
    "plot_data = plot_data.melt(id_vars=['yearweek'], value_vars=['rides','pred'])\n",
    "fig = px.line(plot_data, x='yearweek', y='value', color='variable')\n",
    "fig.add_vline(x=train_size, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis=dict(type='category'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it sounds like (from Claude) that I should fit the model on the whole dataset,\n",
    "and that the correlation-gram plot will tell me if it really is auto-regressive,\n",
    "and that I should check it is stationary (which the mean is and the variance is,\n",
    "except the variance has a periodicity that I should add to the model), and\n",
    "that the residuals aren't auto-regressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(1) + Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['is_holiday'] = model_data.yearweek.str.extract(r\"-(\\d{2})\").astype(int)\n",
    "model_data['is_holiday'] = model_data['is_holiday'].isin([1,47,51,52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar1h = AutoReg(model_data['rides'], 1, exog=model_data['is_holiday']).fit()\n",
    "print(ar1h.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([model_data, \n",
    "                       pd.Series(ar1.predict(),name='AR1'),\n",
    "                       pd.Series(ar1h.predict(),name='AR1H')], axis=1) \\\n",
    "    .melt(id_vars='yearweek', value_vars=['rides','AR1','AR1H'])\n",
    "fig = px.line(plot_data, x='yearweek', y='value', color='variable', title='Observed vs Predicted')\n",
    "fig.update_layout(xaxis=dict(type='category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = ar1h.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_data = pd.concat([model_data, pd.Series(ar1h.resid, name='resid')], axis=1).dropna(how='any')\n",
    "ar1hr = AutoReg(resid_data['resid'],1).fit()\n",
    "print(ar1hr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like there is still auto correlation in the residuals. \n",
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = ar1hr.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exactly what I expect, but shows that maybe the model is under-specified.\n",
    "plot_data = sm.stats.acorr_ljungbox(ar1h.resid, lags=52, return_df=True)\n",
    "plot_data['significant'] = plot_data['lb_pvalue'] < .1\n",
    "px.bar(plot_data, y='lb_stat', color='significant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2 = AutoReg(model_data['rides'], 2, exog=model_data['is_holiday']).fit()\n",
    "print(ar2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([model_data, \n",
    "                       pd.Series(ar1h.predict(),name='AR1H'),\n",
    "                       pd.Series(ar2.predict(),name='AR2')], axis=1) \\\n",
    "    .melt(id_vars='yearweek', value_vars=['rides','AR1H','AR2'])\n",
    "fig = px.line(plot_data, x='yearweek', y='value', color='variable', title='Observed vs Predicted')\n",
    "fig.update_layout(xaxis=dict(type='category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = ar2.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exactly what I expect, but shows that maybe the model is under-specified.\n",
    "plot_data = sm.stats.acorr_ljungbox(ar2.resid, lags=52, return_df=True)\n",
    "plot_data['significant'] = plot_data['lb_pvalue'] < .1\n",
    "px.bar(plot_data, y='lb_stat', color='significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AIC and BIC indicate that AR2 > AR1H > AR1\n",
    "{\"AR1\": (ar1.bic, ar1.aic),\n",
    " \"AR1H\": (ar1h.bic, ar1h.aic),\n",
    " \"AR2\": (ar2.bic, ar2.aic)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(2) Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model seems to require 0-indexed data\n",
    "model_data_delta = model_data.assign(\n",
    "    delta_rides = model_data['rides'].diff()\n",
    ").iloc[1:].reset_index(drop=True)\n",
    "ar2d = AutoReg(model_data_delta['delta_rides'], 2, exog=model_data_delta['is_holiday']).fit()\n",
    "print(ar2d.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AIC and BIC are even lower here. Theoretically I'm not sure if we want to model Y or delta_Y.\n",
    "\n",
    "Also at this point I'm thinkink the estimates for DNC will have a lot of variance because we only have one observation\n",
    "for the DNC=1 days. \n",
    "\n",
    "According to Claude, I should try:\n",
    "\n",
    "* Power analysis: like R's `pwr`\n",
    "* Variance Inflation Factor: which tests multicolinearity which is exacerabated by small samples (I guess because you dont have enough variance)\n",
    "* Rule of thumb: for logistic regression is 10 obs per value\n",
    "* Leave one out cross validation: tests for sensitivity to sample values. use R's `DFBeta` function which tells you how much the coefficient changes when you leave out each observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(1) Delta (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefs are smaller and changed sign now and the AR coef is bigger though.\n",
    "# I'm not sure if this is better or worse.\n",
    "delta_ar1 = AutoReg(model_data_delta['delta'], 1)\n",
    "delta_ar1 = delta_ar1.fit()\n",
    "print(delta_ar1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its of course not going to do much much better because theres only randomness to explain now.\n",
    "plot_data = pd.concat([model_data_delta, pd.Series(delta_ar1.predict(),name='DAR1')], axis=1) \\\n",
    "    .melt(id_vars='yearweek', value_vars=['delta','DAR1'])\n",
    "px.line(plot_data, x='yearweek', y='value', color='variable', title='Observed vs Predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is still ok but the residuals are slightly bigger than before.\n",
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = delta_ar1.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(2)T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woldridge suggests adding a time trend:\n",
    "\n",
    "$$ Y_t = \\vec \\beta [1, X_t, t] + u_t$$\n",
    "\n",
    "this helps mitigate spurious correlations along time, because you allow Y to vary with time directly, \n",
    "meaning your X's now have to affect Y independent of time.\n",
    "\n",
    "Woldridge also says that this model is equivalent to de-trending the data:\n",
    "ie. if you first regressed $y \\sim t$ then the residuals as $\\hat y \\sim x$\n",
    "you recover the exact same $\\hat \\beta$. Therefore there is no need to explicitly difference Y.\n",
    "\n",
    "Woldridge also says it's good to include $\\beta t$ when any $X$ is trending, even if $y_t$ isn't.\n",
    "Because the variation of $x_t$ about its trend might still affect $y_t$, but this variation might be\n",
    "small compared to the trend. Estimating the trend partials out $\\beta x_t$ allowing you to estimate it\n",
    "but other wise $\\beta x_t$ will be swamped by neutralizing the trend part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2t = AutoReg(model_data['rides'], 2, exog=model_data['is_holiday'], trend='ct').fit()\n",
    "print(ar2t.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([model_data, \n",
    "                       pd.Series(ar2.predict(),name='AR2'),\n",
    "                       pd.Series(ar2t.predict(),name='AR2T')], axis=1) \\\n",
    "    .melt(id_vars='yearweek', value_vars=['rides','AR2','AR2T'])\n",
    "fig = px.line(plot_data, x='yearweek', y='value', color='variable', title='Observed vs Predicted')\n",
    "fig.update_layout(xaxis=dict(type='category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9)) \n",
    "fig = ar2t.plot_diagnostics(fig=fig, lags=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AIC and BIC indicate that AR2T > AR2\n",
    "{\"AR2\": (ar2.bic, ar2.aic),\n",
    " \"AR2T\": (ar2t.bic, ar2t.aic)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: Woldridge says that lots of published (nominally seasonal) data is \n",
    "already seasonally adjusted (de-seasoned) as standard practice. eg GDP.\n",
    "\n",
    "You're allowed to include seasonal and trend variables, because the seasonal \n",
    "variable operates at a higher frequency. eg sales within are dominated by seasonality \n",
    "while sales across a decade are dominated by time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['season'] = model_data['yearweek'].str.extract(r'-(\\d{2})').astype(int)\n",
    "model_data['season'] = model_data['season'] // 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like we dont need a seasonality term.\n",
    "ar2s = AutoReg(model_data['rides'], 2, exog=model_data[['is_holiday','season']].astype(int), trend='ct').fit()\n",
    "print(ar2s.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't look like there's any noticeable spike in transit due to the concert.\n",
    "\n",
    "DNC_ISOC_START = datetime.datetime.strptime(DNC_START, YMD).isocalendar()\n",
    "DNC_ISOC_END = datetime.datetime.strptime(DNC_END, YMD).isocalendar()\n",
    "\n",
    "fig = px.line(model_data, x='yearweek', y='rides')\n",
    "fig.add_vline(x=\"{}-{}\".format(DNC_ISOC_START.year, DNC_ISOC_START.week), line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_vline(x=\"{}-{}\".format(DNC_ISOC_END.year, DNC_ISOC_END.week), line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis=dict(type='category'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Woldridge 12.2, we would want to test to show there is not serial correlation of the error terms\n",
    "to be confident in our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "Need to decide which model, DiD, FE, MLM to use on more theoretical grounds, rather than fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selection bias, obviously!\n",
    "    * DNC attendees in Chicago is EXOG\n",
    "        * But we know beforehand they will be concentrated in certain areas.\n",
    "        * We can consider the UC to be the treatment area. \n",
    "            * But there is spillover (SUTVA violation!):\n",
    "                * To airports\n",
    "                * To adjacent sites\n",
    "                * To the city more dispersedly\n",
    "    * Imbalanced size of treatment and control groups:\n",
    "        * DNC only happens during one week\n",
    "        * potentially DNC only affects small parts of city\n",
    "    * Selection on observables\n",
    "        * Can we pick better controls via matching?\n",
    "    * Exogeneity of treatment\n",
    "        * Treatment should be assigned independent of potential outcomes\n",
    "        * otoh the UC and MP are transit-accessible by design!\n",
    "        * => this should inflate the UC coefs\n",
    "* Independent panel units\n",
    "    * Train, bus, and divvy stops might be co-located. How should we think of this?\n",
    "        * e.g. if you take multiple transit legs so co-located ridership are correlated.\n",
    "    * Spatial auto-correlation of rides across stops:\n",
    "        * some baseline % of people take multiple lines, so some % of rides are correlated with additional rides some distance away\n",
    "* Stable composition\n",
    "    * The Damen Green Line opened a week before the event\n",
    "        * Might be able to compare an after-period to parse out the contribution of this stop.\n",
    "* Anticipation effects\n",
    "    * The whole city prepared for years to implement this plan, so they probably did increase transit capacity both systemwide during the leadup to the treatment period and especially at UC and MP. (e.g. the Damen Green Line opened the week before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential outcomes\n",
    "\n",
    "If we frame this as a potential outcomes, we would say:\n",
    "\n",
    "$$\n",
    "outcome =\n",
    "\\begin{cases} \n",
    "    \\text{DNC rides} & \\text{if} & \\text{DNC affected} \\\\\n",
    "    \\text{non-DNC rides} & \\text{if} & \\text{not DNC affected} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive comparison would look like:\n",
    "$$\n",
    "\\text{observed difference in rides} = \\text{ATE on treated} + \\text{selection bias}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[\\text{rides}_i | \\text{DNC}_i = 1] - E[\\text{rides}_i | \\text{DNC}_i = 0]\n",
    "&= E[\\text{rides}_{1i} | \\text{DNC}_i = 1] - E[\\text{rides}_{0i} | \\text{DNC}_i = 1] \\\\\n",
    "&+ E[\\text{rides}_{0i} | \\text{DNC}_i = 1] - E[\\text{rides}_{0i} | \\text{DNC}_i = 0] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The ATE on treated is the difference in potential outcomes among those actually selected for treatment.\n",
    "So in this case it is the difference in rides during the DNC vs the counterfactual rides if the DNC didn't happen.\n",
    "\n",
    "The selection bias is the difference in counterfactual rides if the DNC didn't happen vs observed rides not affected by DNC.\n",
    "I'm not actually sure if this would be positive or negative. However the UC generally has a lot of rides, I think,\n",
    "so I'm guessing a positive selection bias?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIA asserts that Y is independent of D when you condition on X ie. $E[Y | X,D] = E[Y|X]$. \n",
    "\n",
    "It implies that selection bias disappears once you condition on X. This obviously isn't always true. It's mostly used to  motivate textbook naive estimators.\n",
    "\n",
    "In an observational study it asserts that D is as-good-as-random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "Decide whether to keep all stations as controls or to only keep a subset. \n",
    "Remember a control should be comparable, so it can't just be a random sample of other routes.\n",
    "Maybe other central routes?\n",
    "\n",
    "Maybe use some kind of quantile/matching procedure? How should that work should\n",
    "they be similar on facts or outcomes prior to the treatment period? Like should\n",
    "we take all the stations with total ridership similar to this one in the years prior?\n",
    "\n",
    "TODO!\n",
    "\n",
    "Also decide what other controls to add. Like maybe distance from city center? Or neighborhood?\n",
    "\n",
    "TODO!\n",
    "\n",
    "How would we construct Ubers as a panel? Since they are point-to-point, how should we spatially aggregate them?\n",
    "\n",
    "* We could snap them to the current panel? \n",
    "    * But I'm sure most points don't intersect a station. That's kind of the point of rideshares.\n",
    "* We could snap them to the nearest panel?\n",
    "* We could aggregate the panel to census tracts?\n",
    "    * This would ameliorate the stop co-location issue.\n",
    "    * And would significantly decrease the data size. Though bus routes would be difficult to map.\n",
    "* *Actually the Uber data is anonymized to Census Tract or Community Area level!*\n",
    "* It's also HUGE. 144M ROWS!\n",
    "\n",
    "TODO!\n",
    "Read over Woldridge Ch 13 and 14 to see if panel data makes sense in this context.\n",
    "Note that we only have one observation still, so do we have enough variation since\n",
    "we might need to use clustered errors. And if we are at the neighborhood level\n",
    "we might only have one member of the treated panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if it applies here but I wanted to know how to basically run separate regressions per covariate value. For this example we had train and bus data and I wanted a separate intercept and slope of DNC for train and bus. \n",
    "\n",
    "But I wasn't sure what the correct way to do this is. Do you just run `rides ~ train*DNC` or  do you need to run two separate regressions? \n",
    "\n",
    "This took me into Mixed Linear Effects models which you seem to run when you have non-independence between observed units, for example if you have multiple obsevations per subject where each subject has its own idiosyncratic intercept, or like each survey question might have its own idiosyncratic mean and or slope. (You can choose if you want to model just random intercepts or if you want to interact the interesting regressors with the grouping column to get group-specific slopes. Some authors advise to \"keep it maximal\" in an actual controlled experiment and to use all covariates justified by the design.) The idea is you can explicitly model the between group variation, which helps give you more precise estimators on your other regressors. But you don't actually care about the individual group means -- I think it models it but doesn't report it. \n",
    "\n",
    "This method uses the terminiology \"fixed effect\" for the regressors you care about and \"random effect\" for the regressors that indicate these groups. This is a confusing terminology because it sounds like a \"fixed effect\" model where you add a dummy variable per group. However the MLEM random effects are a deviation from the sample average, and again you don't report the group coefficients, just the variance of the coefs. Proper fixed effects models report the per group coefficient. The MLEM random effects basically model the grouping column as a normal distribution where each group's mean is sampled from this distribution. This allows you to model hundreds/thousands of groups. The \"fixed effects\" model doesn't assume the groups have a common distribution. And since it estimates an actual coefficient per group, you can't have high-cardinality groups. \n",
    "\n",
    "MLEM uses \"partial pooling\" where it estimates the group-level intercepts as a variance-weighted average of the group vs population means. \n",
    "\n",
    "$$\n",
    "\\hat b_j = \\frac{\\sigma^2_{pop}}{\\sigma^2_{pop} + \\sigma^2_j / n_j}(\\bar y_j - \\bar y_{pop})\n",
    "$$\n",
    "\n",
    "so when $n_j$ (group size) is large, the weight approaches unity so the estimate tends\n",
    "towards the group deviation from the population: $\\hat b_j \\to (\\bar y_j - \\bar y_{pop})$\n",
    "otoh when $n_j$ is small, the $\\sigma^2_j$ pulls the overall weight to zero, making \n",
    "$\\hat b_j \\to 0$ meaning no deviation from the population mean.\n",
    "\n",
    "This shows how MLEM is different from a true \"fixed effects\" model because it incorporates info from all groups, while the fixed effects models estimates each (j-1) group independently (vs a baseline/reference group). Crucially the FE doesn't borrow strength from other groups: it takes the group differences as-is which makes it prone to high variance in its coefficients estimates particularly in groups with few observations.\n",
    "\n",
    "I was also researching if this is different from clustered standard errors. Clustered \n",
    "standard errors don't actually estimate separate coefficients per group. Instead they\n",
    "correct the standard errors on the population coefficients to account for non-independence\n",
    "(ie correlation in the errors) via grouping. So basically I guess it makes your\n",
    "standard errors bigger because you have less independent information than you think you have.\n",
    "This method applies to situations where you don't care about group-specific means\n",
    "because you don't actually think the means necessarily are different across groups,\n",
    "but you do think the residuals are correlated per group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be able to model this as\n",
    "\n",
    "$$\n",
    "\\text{rides}_{it} = \\alpha_0 + \\beta_0 \\text{rides}_{i,t-1} + \\beta_1 t + \\gamma_0 (1|i) + \\gamma_i +\n",
    "\\delta_0 \\text{DNC}_t + \\delta_1 \\text{perimeter}_i + \\delta_2 \\text{DNC}_t \\text{perimeter}_i + u_{it}\n",
    "$$\n",
    "\n",
    "So as a DiD model ($\\delta$) and optionally an AR(1) process ($\\beta$) and optionally a MLEM ($\\gamma$) or a fixed effect ($\\gamma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Panel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "\n",
    "I have a lot of notes in this section! \n",
    "\n",
    "\n",
    "We're allowed to have dummy intercepts per time period in this framework. ie\n",
    "\n",
    "$$ y_{it} = ... + \\delta_1 \\mathbb{1}_{t=1} + \\delta_2 \\mathbb{1}_{t=2} + ...$$\n",
    "\n",
    "this is preferred when T is small, rather than having an actual time covariate $\\beta t$\n",
    "which is less flexible.\n",
    "\n",
    "For convenience, t=3 and onwards instead of using $\\delta_2 \\Delta \\mathbb{1}_{t=2}$ etc.\n",
    "because those delta indicators flip from 0 to 1 to -1 to 0 whereas the dummies are simply just 0 1 0.\n",
    "\n",
    "These models require $Cov(x_{itj}, u_{it}) = 0$ which is broken if $x_{itj}$ is a lagged dependent variable, especially if it is auto-regressive. Having more time periods does not solve this.\n",
    "\n",
    "ICBST if $u_{it}$ is uncorrelated over time, then $\\Delta u_{it}$ and $\\Delta u_{i,t+1}$ are correlated as -.5 such as when $u_{it}$ is AR(1) but if $u_{it}$ is a random walk then it is ok. We can test for this by getting the residuals on the un-differenced model, differencing to compute $r_{it} = \\Delta u_{it}$, and running simple OLS (NOT an actual AR model) as $r_{it} = \\rho r_{i,t-1} + e_{it}$ and testing $H_0: \\rho = 0$.\n",
    "\n",
    "Woldridge (p. 474) says FD can be really bad if you have measurement error, because differencing reduces X's variation relative to its error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO! Continue reading Woldridge chapter 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO! Incorporate airports as quasi-treatment parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

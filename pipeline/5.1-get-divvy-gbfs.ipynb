{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denormalized ride start/stop points are horribly imprecise. I had assumed\n",
    "they are the actual station locations, but now I wonder if they\n",
    "are user locations when they start/stop on the app. The per-station location\n",
    "imprecision is as big as the between-station distances.\n",
    "\n",
    "So far I haven't trusted the over-time stability of station ids names and locations.\n",
    "But in this notebook we'll use the GBFS station info feed as our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from data.constants import (WORLD_CRS,\n",
    "                            DIVVY_STATIONS_GBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations_in = \"../data/raw/bike_stations_gbfs.json\"\n",
    "bike_stations_out = \"../data/interim/bike_stations_gbfs.geoparquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations_raw = requests.get(DIVVY_STATIONS_GBFS).json()\n",
    "bike_stations = pd.DataFrame.from_records(bike_stations_raw['data']['stations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_cols = ['station_id','capacity','rental_uris','region_id','address', 'lon','lat']\n",
    "bike_stations = (bike_stations\n",
    "                 .assign(geometry = gpd.points_from_xy(bike_stations['lon'],\n",
    "                                                      bike_stations['lat'], \n",
    "                                                      crs=WORLD_CRS))\n",
    "                 .drop(columns=unused_cols)\n",
    "                 .rename(columns={'name':'station_name',\n",
    "                                  'short_name':'station_id'})\n",
    "                .pipe(gpd.GeoDataFrame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 1 stations with non-PK id,name\n"
     ]
    }
   ],
   "source": [
    "# id,name is the composite PK\n",
    "assert all(bike_stations.groupby(['station_id','station_name'],dropna=False).size() == 1)\n",
    "# id to name is at least m:1\n",
    "assert all(bike_stations.groupby(['station_id']).station_name.nunique() == 1)\n",
    "# name to id is 1:Null or 1:1\n",
    "mask = bike_stations.duplicated(subset=['station_name','geometry'])\n",
    "print(f\"Dropping {mask.sum()} stations with non-PK id,name\")\n",
    "bike_stations = bike_stations[~mask]\n",
    "assert all(bike_stations.groupby(['station_name']).station_id.agg(lambda x: x.isna().all() or x.dropna().nunique() == 1))\n",
    "# id is Null or unique\n",
    "assert not bike_stations['station_id'].dropna().duplicated().any()\n",
    "# name is unique given id not null\n",
    "assert not bike_stations.dropna()['station_name'].duplicated().any()\n",
    "# name is the more complete id\n",
    "assert bike_stations.station_name.nunique() > bike_stations['station_id'].nunique()\n",
    "assert bike_stations.station_name.notna().mean() > bike_stations.station_id.notna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "# when name to geo is m:m, the rows are missing ids.\n",
    "# we can prioritize the rows with ids here.\n",
    "# therefore name to geo is at least m:1\n",
    "stable_geos = bike_stations.groupby('station_name').geometry.transform('nunique') == 1\n",
    "\n",
    "assert all(bike_stations[~stable_geos].groupby('station_name')['station_id'].agg(lambda x: x.dropna().nunique()) == 1)\n",
    "\n",
    "print(\"Dropping {} duplicate rows.\".format(sum(~(stable_geos | bike_stations.station_id.notna()))))\n",
    "bike_stations = bike_stations[stable_geos | bike_stations.station_id.notna()]\n",
    "\n",
    "# Now name is unique\n",
    "assert not bike_stations['station_name'].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bike_stations_in, \"w\") as f:\n",
    "    json.dump(bike_stations_raw, f)\n",
    "\n",
    "bike_stations.to_parquet(bike_stations_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

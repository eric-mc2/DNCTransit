{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.constants import (DATA_FOLDER, L_RIDERSHIP_TABLE, BUS_RIDERSHIP_TABLE)\n",
    "from data.cta import CTAClient\n",
    "from data.divvy import DivvyClient\n",
    "from data.uber import UberClient\n",
    "from data.datemath import iso_to_ymd, is_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides_out = os.path.join(DATA_FOLDER, \"raw\", \"train_rides.csv\")\n",
    "bus_rides_out = os.path.join(DATA_FOLDER, \"raw\", \"bus_rides.csv\")\n",
    "bike_rides_out = os.path.join(DATA_FOLDER, \"raw\", \"bike_rides.geoparquet\")\n",
    "uber_rides_out = os.path.join(DATA_FOLDER, \"raw\", \"uber_rides.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n",
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "cta_client = CTAClient(60)\n",
    "divvy_client = DivvyClient()\n",
    "uber_client = UberClient(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline In\n",
    "\n",
    "(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these tables are rather large so we need to make good choices about\n",
    "what to pull in. We should abstract any logic that we might need to re-do\n",
    "if we want to pull in additional dates, and cache anything that takes a while to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, we use models with -1 week, -1 month, and -1 YTD, at daily granularity.\n",
    "\n",
    "Therefore we will pull in data from JANUARY 1, 2024 through AUGUST 31, 2024.\n",
    "\n",
    "Note: we want to finish the whole month of August to ensure we have a FULL \n",
    "week of data for the DNC. Otherwise we may mis-infer a weekly/monthly effect of DNC on ridership\n",
    "when actually we just mechanically omitted some days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_iso = dt(2024, 1, 1).isoformat()\n",
    "data_end_iso = dt(2024, 8, 31, 23, 59, 59).isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = cta_client.soda_get_all(L_RIDERSHIP_TABLE, \n",
    "                            select=\"station_id,date,daytype,rides\",\n",
    "                            where=f\"date between '{data_start_iso}' and '{data_end_iso}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_rides = cta_client.soda_get_all(BUS_RIDERSHIP_TABLE, \n",
    "                            select=\"route,date,daytype,rides\",\n",
    "                            where=f\"date between '{data_start_iso}' and '{data_end_iso}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divvy ridership are at the ride granularity, so we need to aggregate to station-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_ridership(trips: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get counts by station and date.\n",
    "    \"\"\"\n",
    "    trips['start_date'] = trips['start_time'].dt.date\n",
    "    trips['end_date'] = trips['end_time'].dt.date\n",
    "    id_cols = ['station_id','date','vintage'] \n",
    "    id_cols += ['geometry'] if any('geometry' in x for x in trips.columns) else []\n",
    "    start_rides = trips.rename(columns=lambda x: x.replace('start_','')) \\\n",
    "                    .groupby(id_cols, as_index=False).size() \\\n",
    "                    .rename(columns={'size': 'start_rides'})\n",
    "    end_rides = trips.rename(columns=lambda x: x.replace('end_','')) \\\n",
    "                    .groupby(id_cols, as_index=False).size() \\\n",
    "                    .rename(columns={'size': 'end_rides'})\n",
    "    rides = start_rides.merge(end_rides, how='outer')\n",
    "    rides['rides'] = rides['start_rides'].fillna(0) + rides['end_rides'].fillna(0)\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: populating bucket paths.\n",
      "DEBUG: reading  s3://divvy-tripdata/202401-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:26, 26.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202402-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:29, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202403-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202404-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:38,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202405-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:46,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202406-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:56,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202407-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:05,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202408-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:14,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: reading  s3://divvy-tripdata/202409-divvy-tripdata.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:24,  9.42s/it]\n"
     ]
    }
   ],
   "source": [
    "bike_rides = divvy_client.s3_bike_trips(dt.fromisoformat(data_start_iso).year, \n",
    "                                        dt.fromisoformat(data_end_iso).year)\n",
    "bike_rides = map(agg_ridership, bike_rides)\n",
    "bike_rides = pd.concat(list(tqdm(bike_rides)), ignore_index=True)\n",
    "bike_rides = bike_rides.loc[(bike_rides['date'] >= dt.fromisoformat(data_start_iso).date()) \\\n",
    "                            & (bike_rides['date'] <= dt.fromisoformat(data_end_iso).date())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rides = gpd.GeoDataFrame(bike_rides, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, according to https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq/about_data\n",
    "# each station contains multiple bike docks. Nevertheless in this data there are \n",
    "# up to thousands of unique Points per station_id. When we agg station_id -> MultiPoint\n",
    "# and compute the area and perimeter of the minimum bounding circle, convex hull, etc,\n",
    "# they are typically small but can be rather large (tens of thousands of feet, millions of ft^2)\n",
    "# therefore I think we should not ever merge on station_id and only rely on X,Y when given\n",
    "\n",
    "# Analysis commented out ...\n",
    "# from shapely import from_wkt\n",
    "# from data.constants import LOCAL_CRS, WORLD_CRS\n",
    "# from shapely.geometry import MultiPoint\n",
    "# from shapely import minimum_bounding_circle, minimum_rotated_rectangle\n",
    "\n",
    "# bike_rides['geometry'] = bike_rides['geometry'].apply(from_wkt)\n",
    "# bike_rides = gpd.GeoDataFrame(bike_rides, geometry='geometry', crs=WORLD_CRS).to_crs(LOCAL_CRS)\n",
    "\n",
    "# station_geos = bike_rides.groupby('station_id')['geometry'].agg(['nunique',lambda x: MultiPoint(list(set(x)))])\n",
    "\n",
    "# station_geos.columns = ['n','points']\n",
    "# station_geos['circle'] = station_geos['points'].apply(minimum_bounding_circle)\n",
    "# station_geos['rect'] = station_geos['points'].apply(minimum_rotated_rectangle)\n",
    "# station_geos['hull'] = station_geos['points'].apply(lambda x: x.convex_hull)\n",
    "\n",
    "# print(station_geos['n'].describe())\n",
    "\n",
    "# station_geos[['circle','rect','hull']].map(lambda x: x.area).agg('mean',axis=1).hist()\n",
    "# station_geos[['circle','rect','hull']].map(lambda x: x.length).agg('mean',axis=1).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_pickups = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_start_timestamp) as start_date, \n",
    "                                    pickup_census_tract,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"start_date, pickup_census_tract\",\n",
    "                                    pickup=True)\n",
    "uber_dropoffs = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_end_timestamp) as end_date, \n",
    "                                    dropoff_census_tract,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"end_date, dropoff_census_tract\",\n",
    "                                    pickup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_pickups = uber_pickups.rename(columns={'start_date':'date', 'pickup_census_tract':'tract', 'rides':'start_rides'})\n",
    "uber_dropoffs = uber_dropoffs.rename(columns={'end_date':'date', 'dropoff_census_tract':'tract', 'rides':'end_rides'})\n",
    "uber_pickups['tract'] = pd.to_numeric(uber_pickups['tract'], 'coerce')\n",
    "uber_dropoffs['tract'] = pd.to_numeric(uber_dropoffs['tract'], 'coerce')\n",
    "uber_rides = uber_pickups.merge(uber_dropoffs, how='outer')\n",
    "uber_rides['rides'] = uber_rides['start_rides'].fillna(0) + uber_rides['end_rides'].fillna(0)\n",
    "uber_rides['date'] = uber_rides['date'].apply(lambda x: iso_to_ymd(x) if is_iso(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides.to_csv(train_rides_out, index=False)\n",
    "bus_rides.to_csv(bus_rides_out, index=False)\n",
    "bike_rides.to_parquet(bike_rides_out, index=False)\n",
    "uber_rides.to_parquet(uber_rides_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

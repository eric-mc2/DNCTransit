{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from data.constants import (L_RIDERSHIP_TABLE, BUS_RIDERSHIP_TABLE)\n",
    "from data.cta import CTAClient\n",
    "from data.divvy import DivvyClient\n",
    "from data.uber import UberClient\n",
    "from data.datemath import iso_to_ymd, is_iso, to_ymd, is_ymd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides_out = \"../data/raw/train_rides.csv\"\n",
    "bus_rides_out = \"../data/raw/bus_rides.csv\"\n",
    "bike_rides_out = \"../data/raw/bike_rides.geoparquet\"\n",
    "uber_tract_rides_out = \"../data/raw/uber_tract_rides.parquet\"\n",
    "uber_comm_rides_out = \"../data/raw/uber_comm_rides.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_client = CTAClient(60)\n",
    "divvy_client = DivvyClient()\n",
    "uber_client = UberClient(60*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline In\n",
    "\n",
    "(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these tables are rather large so we need to make good choices about\n",
    "what to pull in. We should abstract any logic that we might need to re-do\n",
    "if we want to pull in additional dates, and cache anything that takes a while to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, we use models with -1 week, -1 month, and -1 YTD, at daily granularity.\n",
    "\n",
    "Therefore we will pull in data from JANUARY 1, 2024 through AUGUST 31, 2024.\n",
    "\n",
    "Note: we want to finish the whole month of August to ensure we have a FULL \n",
    "week of data for the DNC. Otherwise we may mis-infer a weekly/monthly effect of DNC on ridership\n",
    "when actually we just mechanically omitted some days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_iso = dt(2024, 1, 1).isoformat()\n",
    "data_end_iso = dt(2024, 8, 31, 23, 59, 59).isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = cta_client.soda_get_all(L_RIDERSHIP_TABLE, \n",
    "                            select=\"station_id,date,daytype,rides\",\n",
    "                            where=f\"date between '{data_start_iso}' and '{data_end_iso}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_rides = cta_client.soda_get_all(BUS_RIDERSHIP_TABLE, \n",
    "                            select=\"route,date,daytype,rides\",\n",
    "                            where=f\"date between '{data_start_iso}' and '{data_end_iso}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divvy ridership are at the ride granularity, so we need to aggregate to station-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_ridership(trips: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get counts by station and date.\n",
    "    \"\"\"\n",
    "    trips['start_date'] = trips['start_time'].dt.date.apply(to_ymd)\n",
    "    trips['end_date'] = trips['end_time'].dt.date.apply(to_ymd)\n",
    "    id_cols = ['station_id','station_name','date','vintage'] \n",
    "    id_cols += ['geometry'] if any('geometry' in x for x in trips.columns) else []\n",
    "    start_rides = trips.rename(columns=lambda x: x.replace('start_','')) \\\n",
    "                    .groupby(id_cols, as_index=False).size() \\\n",
    "                    .rename(columns={'size': 'start_rides'})\n",
    "    end_rides = trips.rename(columns=lambda x: x.replace('end_','')) \\\n",
    "                    .groupby(id_cols, as_index=False).size() \\\n",
    "                    .rename(columns={'size': 'end_rides'})\n",
    "    rides = start_rides.merge(end_rides, how='outer')\n",
    "    rides['rides'] = rides['start_rides'].fillna(0) + rides['end_rides'].fillna(0)\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: populating bucket paths.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:41, 13.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# Takes ~3m to run\n",
    "bike_rides = divvy_client.s3_bike_trips(dt.fromisoformat(data_start_iso).year, \n",
    "                                        dt.fromisoformat(data_end_iso).year)\n",
    "bike_rides = map(agg_ridership, bike_rides)\n",
    "bike_rides = pd.concat(list(tqdm(bike_rides)), ignore_index=True)\n",
    "bike_rides = bike_rides.loc[(bike_rides['date'] >= iso_to_ymd(data_start_iso)) & \\\n",
    "                            (bike_rides['date'] <= iso_to_ymd(data_end_iso))]\n",
    "bike_rides = gpd.GeoDataFrame(bike_rides, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, according to https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq/about_data\n",
    "# each station contains multiple bike docks. Nevertheless, the station location\n",
    "# has crap accuracy and precision. Nominally there are 900k unique station points.\n",
    "# From the normalized stations, we expect ~3.7k or fewer stations.\n",
    "# We'll address this in a subsequent notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Strategy**\n",
    "Because the Socrata database is huge and consistently times out (>20m responses),\n",
    "we require a very careful query strategy. We first break all queries into monthly\n",
    "(falling back to daily) sections and then concatenate them. We also break\n",
    "SELECT statements into multiple queries, to reduce the cardinality\n",
    "of GROUP BY, ie separately querying pickups and dropoffs, and separately tracts \n",
    "vs community areas.\n",
    "\n",
    "**Pickups vs Dropoffs**\n",
    "Uber data, like bike data, has ingress and egress locations. Bus and train data\n",
    "only contains ingress. Instead of throwing out egress data, we make the substantive\n",
    "choice to model RIDES per unit, regardless of ingress/egress status. \n",
    "\n",
    "*Scale*:\n",
    "This essentially\n",
    "doubles the overall uber and bike ridership compared to bus and train, which\n",
    "is not an issue for regression because the difference in scale will be accounted\n",
    "for by transit-type fixed effects. \n",
    "\n",
    "*Censorship*:\n",
    "Observing egress conditional on transit type is a form of sample censorship,\n",
    "where Cov(Y = observed | X) > 0. TODO: I need to review my notes about data\n",
    "censorship. AFAIK, it doesn't introduce bias if it's only conditional on X.\n",
    "\n",
    "*Double counting*:\n",
    "Related to the scale issue, this permits double-counting rides that start and\n",
    "end in the same place. I'd argue these double-counts are consistent across\n",
    "transit types. I'll assume the vast majority of train and bus riders do not\n",
    "exit at the same station they board (exceptions include buskers and commuters\n",
    "that quickly double-back.) OTOH, a circuit on a bike makes a lot of sense\n",
    "in a sight-seeing or errands context. And a short uber drive across the neighborhood\n",
    "also sounds plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from requests.exceptions import ReadTimeout\n",
    "from urllib3.exceptions import TimeoutError\n",
    "done = False\n",
    "while not done:\n",
    "    try:\n",
    "        uber_comm_pickups = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_start_timestamp) as start_date, \n",
    "                                    pickup_community_area,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"start_date, pickup_community_area\",\n",
    "                                    pickup=True,\n",
    "                                    tract=False)\n",
    "        uber_comm_dropoffs = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_end_timestamp) as end_date, \n",
    "                                    dropoff_community_area,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"end_date, dropoff_community_area\",\n",
    "                                    pickup=False,\n",
    "                                    tract=False)\n",
    "        uber_tract_pickups = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_start_timestamp) as start_date, \n",
    "                                    pickup_census_tract,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"start_date, pickup_census_tract\",\n",
    "                                    pickup=True,\n",
    "                                    tract=True)\n",
    "        uber_tract_dropoffs = uber_client.soda_get_uber(select=\"\"\"\n",
    "                                    date_trunc_ymd(trip_end_timestamp) as end_date, \n",
    "                                    dropoff_census_tract,\n",
    "                                    count(trip_id) as rides\n",
    "                                    \"\"\",\n",
    "                                    where_start=iso_to_ymd(data_start_iso), \n",
    "                                    where_end=iso_to_ymd(data_end_iso), \n",
    "                                    group=\"end_date, dropoff_census_tract\",\n",
    "                                    pickup=False,\n",
    "                                    tract=True)\n",
    "        done = True\n",
    "    except (ReadTimeout, TimeoutError) as err:\n",
    "        print(\"Read timeout. Retrying in 60s\")\n",
    "        sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix mismatched date formats\n",
    "assert uber_comm_dropoffs.end_date.apply(is_iso).all()\n",
    "assert uber_comm_pickups.start_date.apply(is_iso).all()\n",
    "assert uber_tract_dropoffs.end_date.apply(is_iso).all()\n",
    "\n",
    "uber_comm_dropoffs['end_date'] = uber_comm_dropoffs['end_date'].apply(iso_to_ymd)\n",
    "uber_comm_pickups['start_date'] = uber_comm_pickups['start_date'].apply(iso_to_ymd)\n",
    "uber_tract_dropoffs['end_date'] = uber_tract_dropoffs['end_date'].apply(iso_to_ymd)\n",
    "\n",
    "# Selectively transform inconsistent formats in uber_tract_pickups:\n",
    "uber_tract_pickups['start_date'] = np.where(\n",
    "    uber_tract_pickups['start_date'].apply(is_ymd),\n",
    "    uber_tract_pickups['start_date'],\n",
    "    uber_tract_pickups['start_date'].apply(iso_to_ymd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for combining pickups and dropoffs.\n",
    "# Note: combined frame will be grouped by DATE and GEO\n",
    "# and will have three rides columns: start, end, total\n",
    "# I choose to preserve start/end to permit a robustness check \n",
    "# that only models start rides to have exact parity with train and bus.\n",
    "uber_tract_pickups = uber_tract_pickups.rename(columns={'start_date':'date', \n",
    "                                            'pickup_census_tract':'tract', \n",
    "                                            'rides':'start_rides'})\n",
    "uber_tract_dropoffs = uber_tract_dropoffs.rename(columns={'end_date':'date', \n",
    "                                              'dropoff_census_tract':'tract', \n",
    "                                              'rides':'end_rides'})\n",
    "uber_comm_pickups = uber_comm_pickups.rename(columns={'start_date':'date', \n",
    "                                            'pickup_community_area':'comm', \n",
    "                                            'rides':'start_rides'})\n",
    "uber_comm_dropoffs = uber_comm_dropoffs.rename(columns={'end_date':'date', \n",
    "                                              'dropoff_community_area':'comm',\n",
    "                                              'rides':'end_rides'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep tracts and comms as separate dfs since they are separate units of agg.\n",
    "# Just like the stations vs lines df.\n",
    "uber_tract_rides = uber_tract_pickups.merge(uber_tract_dropoffs, how='outer')\n",
    "uber_comm_rides = uber_comm_pickups.merge(uber_comm_dropoffs, how='outer')\n",
    "uber_tract_rides['rides'] = uber_tract_rides['start_rides'].fillna(0) + uber_tract_rides['end_rides'].fillna(0)\n",
    "uber_comm_rides['rides'] = uber_comm_rides['start_rides'].fillna(0) + uber_comm_rides['end_rides'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides.to_csv(train_rides_out, index=False)\n",
    "bus_rides.to_csv(bus_rides_out, index=False)\n",
    "bike_rides.to_parquet(bike_rides_out, index=False)\n",
    "uber_tract_rides.to_parquet(uber_tract_rides_out, index=False)\n",
    "uber_comm_rides.to_parquet(uber_comm_rides_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

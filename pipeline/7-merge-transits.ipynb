{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "from data.constants import DATA_FOLDER, WORLD_CRS, LOCAL_CRS, CHI_BOUNDARY_FILE\n",
    "from data.datemath import to_ymd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_in = os.path.join(DATA_FOLDER, \"interim\", \"train_stations.geojson\")\n",
    "bus_routes_file_in = os.path.join(DATA_FOLDER, \"interim\", \"bus_routes.geojson\")\n",
    "bus_stops_file_in = os.path.join(DATA_FOLDER, \"interim\", \"bus_stops.geojson\")\n",
    "bike_stations_file_in = os.path.join(DATA_FOLDER, \"interim\", \"bike_stations.geojson\")\n",
    "tract_file_in = os.path.join(DATA_FOLDER, \"interim\", \"tracts.geoparquet\")\n",
    "comm_file_in = os.path.join(DATA_FOLDER, \"interim\", \"communities.geojson\")\n",
    "\n",
    "train_rides_in = os.path.join(DATA_FOLDER, \"interim\", \"train_rides.csv\")\n",
    "bus_rides_in = os.path.join(DATA_FOLDER, \"interim\", \"bus_rides.csv\")\n",
    "bike_rides_in = os.path.join(DATA_FOLDER, \"interim\", \"bike_rides.geoparquet\")\n",
    "uber_rides_in = os.path.join(DATA_FOLDER, \"interim\", \"uber_rides.parquet\")\n",
    "\n",
    "point_panel_out = os.path.join(DATA_FOLDER, \"interim\", \"point_panel.parquet\")\n",
    "line_panel_out = os.path.join(DATA_FOLDER, \"interim\", \"line_panel.parquet\")\n",
    "tract_panel_out = os.path.join(DATA_FOLDER, \"interim\", \"tract_panel.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = gpd.read_file(train_file_in)\n",
    "bus_routes = gpd.read_file(bus_routes_file_in)\n",
    "bus_stops = gpd.read_file(bus_stops_file_in)\n",
    "tract_points = gpd.read_parquet(tract_file_in)\n",
    "comm_points = gpd.read_file(comm_file_in)\n",
    "bike_stations = gpd.read_file(bike_stations_file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = pd.read_csv(train_rides_in)\n",
    "bus_rides = pd.read_csv(bus_rides_in)\n",
    "bike_rides = gpd.read_parquet(bike_rides_in)\n",
    "uber_rides = pd.read_parquet(uber_rides_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about granularity of entrypoints, not line/direction because we dont know\n",
    "# which line/direction the riders go.\n",
    "train_stations = train_stations.drop_duplicates('map_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = train_stations.merge(train_rides, how='right', right_on='station_id', left_on='map_id', validate=\"1:m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate to Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover statistic before exploding\n",
    "lines_per_station = train_stations.set_index('map_id')['line'].str.count(',')+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode multi-line stations to keep correct cardinality of lines\n",
    "train_stations['line'] = train_stations['line'].str.split(',')\n",
    "train_stations = train_stations.explode('line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['airport', 'uc_400', 'uc_800', 'uc_1600', 'mp_400', 'mp_800', 'mp_1600']\n",
    "train_line_features = train_stations.groupby('line')[feature_cols].any().astype(float).reset_index()\n",
    "train_centroids = train_stations.groupby('line')['geometry'].agg(lambda x: MultiPoint(list(x)).centroid)\n",
    "train_line_features['centroid'] = gpd.GeoSeries(train_line_features['line'].map(train_centroids), crs=train_stations.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ridership data isn't at directional granularity, even though some stations \n",
    "# do have separate directional entrances. We will apportion ridership equally per\n",
    "# line at multi-line stations.\n",
    "# TODO: Improve this model of line popularity?\n",
    "train_line_rides = train_rides.assign(rides = train_rides['rides'] / train_rides['map_id'].map(lines_per_station))\n",
    "train_line_rides = train_line_rides.groupby(['line','date','daytype','DNC'],as_index=False).agg({'rides':'sum'})\n",
    "train_line_rides = train_line_rides.merge(train_line_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Double-checking if all bike stations (for the years currently accessed)\n",
    "# #       already have valid (ie chicago?) geometries\n",
    "# from shapely import box\n",
    "# chi_boundary = gpd.read_file(CHI_BOUNDARY_FILE)\n",
    "# chi_bound_geo = chi_boundary['geometry'].to_crs(LOCAL_CRS).iloc[0]\n",
    "# chi_box = box(*chi_bound_geo.bounds) # Using bounding box for faster computation\n",
    "# is_valid = bike_rides['geometry'].to_crs(LOCAL_CRS).within(chi_box)\n",
    "\n",
    "# # \"Invalid\" geos are all in Evanston essentially, which is fine for our purposes.\n",
    "# fig = chi_boundary.boundary.plot()\n",
    "# bike_rides[~is_valid]['geometry'].drop_duplicates().plot(ax=fig, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the current dataset, all id's are integer, so we will truncate decimal part before stringifying\n",
    "assert all(bike_stations['station_id'].astype(int) == bike_stations['station_id'])\n",
    "bike_stations['station_id'] = bike_stations['station_id'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common stations? True\n"
     ]
    }
   ],
   "source": [
    "tmp = bike_stations.merge(bike_rides, on=['station_id','vintage'], how='inner')\n",
    "print(f\"No common stations? {tmp.empty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common points? False. 359714 / 1086142 points in common.\n"
     ]
    }
   ],
   "source": [
    "tmp = bike_stations.merge(bike_rides, on=['geometry'], how='inner')\n",
    "print(f\"No common points? {tmp.empty}. {len(tmp)} / {len(bike_rides)} points in common.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to gain by merging:  {'name'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns to gain by merging: \", set(bike_stations.columns) - set(bike_rides.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string for dtype parity with other transits\n",
    "bike_rides['date'] = bike_rides['date'].apply(to_ymd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "\n",
    "For now, since all data vintages currently pulled are denormalized and already\n",
    "have valid geometries, on which we've already computed our spatial features,\n",
    "there's actually nothing to merge via bike_stations.\n",
    "\n",
    "If you return to this, note that within bike_stations the station_id <-> geometry is not 1:1,\n",
    "even though according to the documentation each station has multiple bike docks.\n",
    "Some of the points are thousands of feet away per station_id.\n",
    "Therefore be careful about how you construct the merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Rides + Stops + Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping stops with unknown tracts (ie outside chicago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops = bus_stops.dropna(subset=['route','tract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since the bus ridership is at route granularity, not stop or point granularity,\n",
    "we need to aggregate to stop-level features to route.\n",
    "\n",
    "This is tricky since routes span miles across the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first thought is to \n",
    "compute the proportion of stops that exhibit a certain feature, ie proportion\n",
    "of stops near the United Center. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['uc_400','uc_800','uc_1600','mp_400','mp_800','mp_1600','airport']\n",
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns out poorly scaled -- most routes\n",
    "have zero stops near the United Center, while those that do tend to have a very\n",
    "small proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of routes with ANY POI stops\n",
      "{'uc_400': 0.03, 'uc_800': 0.06, 'uc_1600': 0.1, 'mp_400': 0.01, 'mp_800': 0.04, 'mp_1600': 0.1, 'airport': 0.08}\n"
     ]
    }
   ],
   "source": [
    "print('Proportion of routes with ANY POI stops')\n",
    "print((bus_stops_features > 0).mean().round(2).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of mean POI stops per route, given route serves ANY POI's:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uc_400</th>\n",
       "      <th>uc_800</th>\n",
       "      <th>uc_1600</th>\n",
       "      <th>mp_400</th>\n",
       "      <th>mp_800</th>\n",
       "      <th>mp_1600</th>\n",
       "      <th>airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0.005408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.055232</td>\n",
       "      <td>0.150561</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.045943</td>\n",
       "      <td>0.093818</td>\n",
       "      <td>0.010453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093375</td>\n",
       "      <td>0.006920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uc_400     uc_800    uc_1600     mp_400     mp_800    mp_1600  \\\n",
       "count  39.000000  39.000000  39.000000  39.000000  39.000000  39.000000   \n",
       "mean    0.005838   0.024061   0.086059   0.001506   0.015386   0.050852   \n",
       "std     0.018715   0.055232   0.150561   0.006970   0.045943   0.093818   \n",
       "min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "25%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "50%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "75%     0.000000   0.000000   0.135339   0.000000   0.000000   0.093375   \n",
       "max     0.086614   0.222222   0.666667   0.039604   0.234375   0.468750   \n",
       "\n",
       "         airport  \n",
       "count  39.000000  \n",
       "mean    0.005408  \n",
       "std     0.010453  \n",
       "min     0.000000  \n",
       "25%     0.000000  \n",
       "50%     0.000000  \n",
       "75%     0.006920  \n",
       "max     0.038462  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of mean POI stops per route, given route serves ANY POI's:\")\n",
    "bus_stops_features[(bus_stops_features>0).agg('any',axis=1)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we will interpret simply as binary whether or not the route serves the POI\n",
    "anywhere along the route, without taking into account the lenght of the route,\n",
    "nor ridership density along the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].any().astype(float)\n",
    "bus_routes = bus_routes.merge(bus_stops_features, how='left', left_on='route', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_centroids = bus_stops.groupby('route')['geometry'].agg(lambda x: MultiPoint(list(x)).centroid)\n",
    "bus_routes['centroid'] = gpd.GeoSeries(bus_routes['route'].map(bus_centroids), crs=bus_stops.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_rides = bus_routes.merge(bus_rides, on='route', how='right', validate='1:m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Rides + Tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "\n",
    "Community areas are kinda big. I'll use tracts and drop all rides that are\n",
    "even anonymized at the tract level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides = uber_rides.dropna(subset=['tract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Uber dataset contains rides that start OR end in chicago, but \n",
    "may include rides that start XOR end outside of chicago. We haven't computed\n",
    "features on those, but for simplicity, we'll set them to false, which is ok \n",
    "for our definitions. Technically, OHare is inside of chicago even if it's \n",
    "surrounded by non-chicago tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides['tract'] = uber_rides['tract'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides = tract_points[feature_cols+['centroid','geoid10']] \\\n",
    "                .merge(uber_rides,\n",
    "                       right_on='tract', left_on='geoid10',\n",
    "                       how='right', validate=\"1:m\") \\\n",
    "                .drop(columns=['geoid10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_panels(bus=None, train=None, bike=None, uber=None):\n",
    "    # prepend categorical transit type and only take common columns\n",
    "    dfs = zip([bus,train,bike,uber],['bus','train','bike','uber'])\n",
    "    dfs, keys = zip(*filter(lambda x: x[0] is not None and not x[0].empty, dfs))\n",
    "    # note: to use keys and still ignore the index, we have to just reset it later.\n",
    "    panel = pd.concat(dfs, ignore_index=False, join='inner',\n",
    "                      keys=keys, names=['transit'])\n",
    "    panel = panel.reset_index(level='transit').reset_index(drop=True)\n",
    "    panel['tid'] = panel['transit'] + \"_\" + panel['id'].astype(str)\n",
    "    # convert geo into xy features. \n",
    "    # must project to compute centroid. unprojecting because WGS scale is nicer for regression\n",
    "    if 'geometry' in panel.columns:\n",
    "        panel['lat'] = gpd.GeoSeries(panel.geometry).to_crs(LOCAL_CRS).centroid.to_crs(WORLD_CRS).y\n",
    "        panel['long'] = gpd.GeoSeries(panel.geometry).to_crs(LOCAL_CRS).centroid.to_crs(WORLD_CRS).x\n",
    "        panel = panel.drop(columns=['geometry'])\n",
    "    return panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All things equal, we should take the bus and uber components with a grain of salt.\n",
    "The uber and bus components have systematically larger measurement error in lat/lon since they\n",
    "are snapped to tract / route centroid. This breaks an OLS assumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_panel = combine_panels(train=train_rides.assign(id = train_rides['station_id'].astype(str)),\n",
    "                            bike=bike_rides.rename(columns={\"station_id\":\"id\"}),\n",
    "                            uber=uber_rides.rename(columns={'centroid':'geometry'}) \\\n",
    "                                .assign(id = uber_rides['tract'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_panel = combine_panels(bus=bus_rides.drop(columns=['geometry']) \\\n",
    "                                 .rename(columns={'centroid':'geometry', 'route':'id'}),\n",
    "                            train=train_line_rides.rename(columns={'centroid':'geometry', 'line':'id'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tract Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tract_point_statistics(tract_df, point_df, transit: str):\n",
    "    tract_centroids = tract_df.set_geometry(tract_df.to_crs(LOCAL_CRS).centroid)\n",
    "\n",
    "    # Note sjoin_nearest is m:m if points are equidistant ==> we further consolidate\n",
    "    access = tract_centroids.sjoin_nearest(point_df.to_crs(LOCAL_CRS), \n",
    "                                           how='left', \n",
    "                                           distance_col=f'{transit}_access') \\\n",
    "                        .groupby('geoid10',as_index=False)[f'{transit}_access'].min()\n",
    "\n",
    "    contained = tract_df.merge(point_df, left_on='geoid10', right_on='tract',\n",
    "                               how='left', validate=\"1:m\") \\\n",
    "                        .groupby('geoid10',as_index=False).size() \\\n",
    "                        .rename(columns={'size':f'{transit}_count'})\n",
    "    \n",
    "    tract_df = tract_df.merge(access).merge(contained)\n",
    "    return tract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_points = tract_points.pipe(tract_point_statistics, train_stations[['tract','geometry']], 'train')\n",
    "tract_points = tract_points.pipe(tract_point_statistics, bike_stations[['tract','geometry']], 'bike')\n",
    "tract_points = tract_points.pipe(tract_point_statistics, bus_stops[['tract','geometry']], 'bus')\n",
    "\n",
    "# These don't have a lot of meaning for uber, particularly because the accessability==0 always\n",
    "# but just throwing in for good measure.\n",
    "tract_points = tract_points.pipe(tract_point_statistics, \n",
    "                                 uber_rides[['tract','centroid']].drop_duplicates().set_geometry('centroid'), 'uber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['tract','date','daytype','DNC']\n",
    "\n",
    "train_rides_by_tract = train_rides[id_cols + ['rides']].groupby(id_cols, as_index=False).sum()\n",
    "bike_rides_by_tract = bike_rides[id_cols + ['start_rides','end_rides','rides']].groupby(id_cols, as_index=False).sum()\n",
    "uber_rides_by_tract = uber_rides[id_cols + ['start_rides','end_rides','rides']].groupby(id_cols, as_index=False).sum()\n",
    "# Handle bus rides separately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bus ridership is only known per route. We will disaggregate to tracts by\n",
    "# assuming equal ridership per stop: ridership per tract ~ nbr stops in tract.\n",
    "stops_per_route_tract = bus_stops.value_counts(['route','tract'])\n",
    "stops_per_route = bus_stops.value_counts(['route'])\n",
    "prop_in_route_tract = stops_per_route_tract / stops_per_route\n",
    "prop_in_route_tract = prop_in_route_tract.rename('prop').reset_index()\n",
    "assert (prop_in_route_tract.groupby('route')['prop'].sum()==1).all()\n",
    "\n",
    "bus_rides_by_tract = bus_rides.merge(bus_stops[['route','tract']]).merge(prop_in_route_tract)\n",
    "bus_rides_by_tract['rides'] = bus_rides_by_tract['rides'] * bus_rides_by_tract['prop']\n",
    "bus_rides_by_tract = bus_rides_by_tract[id_cols + ['rides']].groupby(id_cols, as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_rides = combine_panels(bus=bus_rides_by_tract.rename(columns={\"tract\":\"id\"}),\n",
    "                             train=train_rides_by_tract.rename(columns={\"tract\":\"id\"}),\n",
    "                             bike=bike_rides_by_tract.rename(columns={\"tract\":\"id\"}),\n",
    "                             uber=uber_rides_by_tract.rename(columns={\"tract\":\"id\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_panel = tract_points.merge(tract_rides, right_on='id', left_on='geoid10')\n",
    "tract_panel['lat'] = tract_panel['centroid'].to_crs(LOCAL_CRS).centroid.to_crs(WORLD_CRS).y\n",
    "tract_panel['long'] = tract_panel['centroid'].to_crs(LOCAL_CRS).centroid.to_crs(WORLD_CRS).x\n",
    "tract_panel = tract_panel.drop(columns=['centroid','geometry','geoid10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_panel.to_parquet(point_panel_out, index=False)\n",
    "line_panel.to_parquet(line_panel_out, index=False)\n",
    "tract_panel.to_parquet(tract_panel_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

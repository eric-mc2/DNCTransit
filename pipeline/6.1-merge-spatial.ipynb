{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines_in = \"../data/raw/train_lines.geojson\"\n",
    "train_stations_in = \"../data/interim/train_stations.geojson\"\n",
    "bus_routes_file_in = \"../data/raw/bus_routes.geojson\"\n",
    "bus_stops_file_in = \"../data/interim/bus_stops.geojson\"\n",
    "bike_stations_file_in = \"../data/interim/bike_stations_gbfs_v2.geojson\"\n",
    "tract_file_in = \"../data/interim/tracts.geoparquet\"\n",
    "comm_file_in =  \"../data/interim/communities.geojson\"\n",
    "\n",
    "train_rides_in = \"../data/interim/train_rides.csv\"\n",
    "bus_rides_in = \"../data/interim/bus_rides.csv\"\n",
    "bike_rides_in = \"../data/interim/bike_rides_v3.geoparquet\"\n",
    "uber_rides_in = \"../data/interim/uber_rides.parquet\"\n",
    "\n",
    "train_line_rides_out = \"../data/final/train_line_rides.parquet\"\n",
    "train_station_rides_out = \"../data/final/train_station_rides.parquet\"\n",
    "bus_rides_out = \"../data/final/bus_rides.parquet\"\n",
    "bike_rides_out = \"../data/final/bike_rides.geoparquet\"\n",
    "uber_rides_out = \"../data/final/uber_rides.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = gpd.read_file(train_lines_in)\n",
    "train_stations = gpd.read_file(train_stations_in)\n",
    "bus_routes = gpd.read_file(bus_routes_file_in)\n",
    "bus_stops = gpd.read_file(bus_stops_file_in)\n",
    "tracts = gpd.read_parquet(tract_file_in)\n",
    "comms = gpd.read_file(comm_file_in)\n",
    "bike_stations = gpd.read_file(bike_stations_file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = pd.read_csv(train_rides_in)\n",
    "bus_rides = pd.read_csv(bus_rides_in)\n",
    "bike_rides = gpd.read_parquet(bike_rides_in)\n",
    "uber_rides = pd.read_parquet(uber_rides_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about granularity of entrypoints, not line/direction because we dont know\n",
    "# which line/direction the riders go.\n",
    "train_stations = train_stations.drop_duplicates('map_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = train_stations.merge(train_rides, how='right', right_on='station_id', left_on='map_id', validate=\"1:m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides + Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate station features per line:\n",
    "feature_cols = ['airport', 'uc_400', 'uc_800', 'uc_1600', 'mp_400', 'mp_800', 'mp_1600']\n",
    "\n",
    "# Recover statistic before exploding\n",
    "lines_per_station = train_stations.set_index('map_id')['line'].str.count(',')+1\n",
    "\n",
    "# Explode multi-line stations to keep correct cardinality of lines\n",
    "train_stations['line'] = train_stations['line'].str.split(',')\n",
    "train_stations = train_stations.explode('line')\n",
    "\n",
    "# Agg station -> line\n",
    "train_line_features = train_stations.groupby('line')[feature_cols].any().astype(float).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate ridership and merge line-level features:\n",
    "\n",
    "# The ridership data isn't at directional granularity, even though some stations \n",
    "# do have separate directional entrances. We will apportion ridership equally per\n",
    "# line at multi-line stations.\n",
    "# XXX: Improve this model of line popularity?\n",
    "train_line_rides = train_rides.assign(rides = train_rides['rides'] / train_rides['map_id'].map(lines_per_station))\n",
    "train_line_rides = train_line_rides.groupby(['line','date','dotw','is_weekend','DNC'],as_index=False).agg({'rides':'sum'})\n",
    "train_line_rides = train_line_rides.merge(train_line_features)\n",
    "train_line_rides = train_lines.merge(train_line_rides, how='right', on='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Double-checking if all bike stations (for the years currently accessed)\n",
    "# #       already have valid (ie chicago?) geometries\n",
    "# from shapely import box\n",
    "# chi_boundary = gpd.read_file(CHI_BOUNDARY_FILE)\n",
    "# chi_bound_geo = chi_boundary['geometry'].to_crs(LOCAL_CRS).iloc[0]\n",
    "# chi_box = box(*chi_bound_geo.bounds) # Using bounding box for faster computation\n",
    "# is_valid = bike_rides['geometry'].to_crs(LOCAL_CRS).within(chi_box)\n",
    "\n",
    "# # \"Invalid\" geos are all in Evanston essentially, which is fine for our purposes.\n",
    "# fig = chi_boundary.boundary.plot()\n",
    "# bike_rides[~is_valid]['geometry'].drop_duplicates().plot(ax=fig, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to gain by merging:  {'station_id'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns to gain by merging: \", set(bike_stations.columns) - set(bike_rides.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "\n",
    "For now, since all data vintages currently pulled are denormalized and already\n",
    "have valid geometries, on which we've already computed our spatial features,\n",
    "there's actually nothing to merge via bike_stations.\n",
    "\n",
    "If you return to this, note that within bike_stations the station_id <-> geometry is not 1:1,\n",
    "even though according to the documentation each station has multiple bike docks.\n",
    "Some of the points are thousands of feet away per station_id.\n",
    "Therefore be careful about how you construct the merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Rides + Stops + Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping stops with unknown tracts (ie outside chicago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 1 (0.0%) of rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping {} ({:.1%}) of rows\".format(\n",
    "    bus_stops[['route','tract']].isna().any(axis=1).sum(),\n",
    "    bus_stops[['route','tract']].isna().any(axis=1).mean(),\n",
    "))\n",
    "bus_stops = bus_stops.dropna(subset=['route','tract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since the bus ridership is at route granularity, not stop or point granularity,\n",
    "we need to aggregate to stop-level features to route.\n",
    "\n",
    "This is tricky since routes span miles across the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first thought is to \n",
    "compute the proportion of stops that exhibit a certain feature, ie proportion\n",
    "of stops near the United Center. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['uc_400','uc_800','uc_1600','mp_400','mp_800','mp_1600','airport']\n",
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns out poorly scaled -- most routes\n",
    "have zero stops near the United Center, while those that do tend to have a very\n",
    "small proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of routes with ANY POI stops\n",
      "{'uc_400': 0.03, 'uc_800': 0.05, 'uc_1600': 0.1, 'mp_400': 0.01, 'mp_800': 0.04, 'mp_1600': 0.1, 'airport': 0.07}\n"
     ]
    }
   ],
   "source": [
    "print('Proportion of routes with ANY POI stops')\n",
    "print((bus_stops_features > 0).mean().round(2).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of mean POI stops per route, given route serves ANY POI's:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uc_400</th>\n",
       "      <th>uc_800</th>\n",
       "      <th>uc_1600</th>\n",
       "      <th>mp_400</th>\n",
       "      <th>mp_800</th>\n",
       "      <th>mp_1600</th>\n",
       "      <th>airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.085773</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>0.049139</td>\n",
       "      <td>0.004464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.055232</td>\n",
       "      <td>0.150498</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.091428</td>\n",
       "      <td>0.008463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093375</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uc_400     uc_800    uc_1600     mp_400     mp_800    mp_1600  \\\n",
       "count  39.000000  39.000000  39.000000  39.000000  39.000000  39.000000   \n",
       "mean    0.005838   0.024061   0.085773   0.001223   0.014325   0.049139   \n",
       "std     0.018715   0.055232   0.150498   0.005439   0.043182   0.091428   \n",
       "min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "25%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "50%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "75%     0.000000   0.000000   0.134408   0.000000   0.000000   0.093375   \n",
       "max     0.086614   0.222222   0.666667   0.028571   0.234375   0.468750   \n",
       "\n",
       "         airport  \n",
       "count  39.000000  \n",
       "mean    0.004464  \n",
       "std     0.008463  \n",
       "min     0.000000  \n",
       "25%     0.000000  \n",
       "50%     0.000000  \n",
       "75%     0.006944  \n",
       "max     0.031250  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of mean POI stops per route, given route serves ANY POI's:\")\n",
    "bus_stops_features[(bus_stops_features>0).agg('any',axis=1)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we will interpret simply as binary whether or not the route serves the POI\n",
    "anywhere along the route, without taking into account the lenght of the route,\n",
    "nor ridership density along the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].any().astype(float)\n",
    "bus_routes = bus_routes.merge(bus_stops_features, how='left', left_on='route', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_rides = bus_routes.merge(bus_rides, on='route', how='right', validate='1:m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Rides + Tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "Not dropping anonymized observations (null tract/comm) because they still add to the non-spatial\n",
    "time series count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_tracts = tracts[feature_cols + ['geometry','centroid','geoid10']] \\\n",
    "                [lambda x: x['geoid10'].isin(uber_rides[uber_rides.unit == 'tract']['id'])] \\\n",
    "                .rename(columns={'geoid10':'id'})\n",
    "\n",
    "uber_comms = comms[feature_cols + ['geometry','centroid','comm_area']] \\\n",
    "                [lambda x: x['comm_area'].isin(uber_rides[uber_rides.unit == 'comm']['id'])]\\\n",
    "                .rename(columns={'comm_area':'id'})\n",
    "\n",
    "assert len(uber_rides.columns.intersection(uber_tracts.columns)) == 1\n",
    "assert len(uber_rides.columns.intersection(uber_comms.columns)) == 1\n",
    "\n",
    "uber_tract_rides = uber_tracts.merge(uber_rides[uber_rides.unit == 'tract'], how='right', on='id')\n",
    "uber_comm_rides = uber_comms.merge(uber_rides[uber_rides.unit == 'comm'], how='right', on='id')\n",
    "uber_rides = pd.concat([uber_tract_rides, uber_comm_rides], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides.to_parquet(uber_rides_out)\n",
    "bike_rides.to_parquet(bike_rides_out)\n",
    "train_rides.to_parquet(train_station_rides_out)\n",
    "train_line_rides.to_parquet(train_line_rides_out)\n",
    "bus_rides.to_parquet(bus_rides_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

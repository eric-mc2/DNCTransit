{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines_in = \"../data/raw/train_lines.geojson\"\n",
    "train_stations_in = \"../data/interim/train_stations.geojson\"\n",
    "bus_routes_file_in = \"../data/raw/bus_routes.geojson\"\n",
    "bus_stops_file_in = \"../data/interim/bus_stops.geojson\"\n",
    "bike_stations_file_in = \"../data/interim/bike_stations_gbfs_v2.geojson\"\n",
    "tract_file_in = \"../data/interim/tracts.geoparquet\"\n",
    "comm_file_in =  \"../data/interim/communities.geojson\"\n",
    "\n",
    "train_rides_in = \"../data/interim/train_rides.csv\"\n",
    "bus_rides_in = \"../data/interim/bus_rides.csv\"\n",
    "bike_rides_in = \"../data/interim/bike_rides_v3.geoparquet\"\n",
    "uber_rides_in = \"../data/interim/uber_rides.parquet\"\n",
    "\n",
    "train_line_rides_out = \"../data/final/train_line_rides.parquet\"\n",
    "train_station_rides_out = \"../data/final/train_station_rides.parquet\"\n",
    "bus_rides_out = \"../data/final/bus_rides.parquet\"\n",
    "bike_rides_out = \"../data/final/bike_rides.geoparquet\"\n",
    "uber_rides_out = \"../data/final/uber_rides.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = gpd.read_file(train_lines_in)\n",
    "train_stations = gpd.read_file(train_stations_in)\n",
    "bus_routes = gpd.read_file(bus_routes_file_in)\n",
    "bus_stops = gpd.read_file(bus_stops_file_in)\n",
    "tracts = gpd.read_parquet(tract_file_in)\n",
    "comms = gpd.read_file(comm_file_in)\n",
    "bike_stations = gpd.read_file(bike_stations_file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = pd.read_csv(train_rides_in)\n",
    "bus_rides = pd.read_csv(bus_rides_in)\n",
    "bike_rides = gpd.read_parquet(bike_rides_in)\n",
    "uber_rides = pd.read_parquet(uber_rides_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about granularity of entrypoints, not line/direction because we dont know\n",
    "# which line/direction the riders go.\n",
    "train_stations = train_stations.drop_duplicates('map_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rides = train_stations.merge(train_rides, how='right', right_on='station_id', left_on='map_id', validate=\"1:m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Rides + Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate station features per line:\n",
    "feature_cols = ['airport'] + list(train_stations.filter(regex=r'\\d+m$').columns)\n",
    "\n",
    "# Recover statistic before exploding\n",
    "lines_per_station = train_stations.set_index('map_id')['line'].str.count(',')+1\n",
    "\n",
    "# Explode multi-line stations to keep correct cardinality of lines\n",
    "train_stations['line'] = train_stations['line'].str.split(',')\n",
    "train_stations = train_stations.explode('line')\n",
    "\n",
    "# Agg station -> line\n",
    "train_line_features = train_stations.groupby('line')[feature_cols].any().astype(float).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate ridership and merge line-level features:\n",
    "\n",
    "# The ridership data isn't at directional granularity, even though some stations \n",
    "# do have separate directional entrances. We will apportion ridership equally per\n",
    "# line at multi-line stations.\n",
    "# XXX: Improve this model of line popularity?\n",
    "train_line_rides = train_rides.assign(rides = train_rides['rides'] / train_rides['map_id'].map(lines_per_station))\n",
    "train_line_rides = train_line_rides.groupby(['line','date','dotw','is_weekend','DNC'],as_index=False).agg({'rides':'sum'})\n",
    "train_line_rides = train_line_rides.merge(train_line_features)\n",
    "train_line_rides = train_lines.merge(train_line_rides, how='right', on='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Double-checking if all bike stations (for the years currently accessed)\n",
    "# #       already have valid (ie chicago?) geometries\n",
    "# from shapely import box\n",
    "# chi_boundary = gpd.read_file(CHI_BOUNDARY_FILE)\n",
    "# chi_bound_geo = chi_boundary['geometry'].to_crs(LOCAL_CRS).iloc[0]\n",
    "# chi_box = box(*chi_bound_geo.bounds) # Using bounding box for faster computation\n",
    "# is_valid = bike_rides['geometry'].to_crs(LOCAL_CRS).within(chi_box)\n",
    "\n",
    "# # \"Invalid\" geos are all in Evanston essentially, which is fine for our purposes.\n",
    "# fig = chi_boundary.boundary.plot()\n",
    "# bike_rides[~is_valid]['geometry'].drop_duplicates().plot(ax=fig, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to gain by merging:  {'station_id'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns to gain by merging: \", set(bike_stations.columns) - set(bike_rides.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "\n",
    "For now, since all data vintages currently pulled are denormalized and already\n",
    "have valid geometries, on which we've already computed our spatial features,\n",
    "there's actually nothing to merge via bike_stations.\n",
    "\n",
    "If you return to this, note that within bike_stations the station_id <-> geometry is not 1:1,\n",
    "even though according to the documentation each station has multiple bike docks.\n",
    "Some of the points are thousands of feet away per station_id.\n",
    "Therefore be careful about how you construct the merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Rides + Stops + Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping stops with unknown tracts (ie outside chicago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 1 (0.0%) of rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping {} ({:.1%}) of rows\".format(\n",
    "    bus_stops[['route','tract']].isna().any(axis=1).sum(),\n",
    "    bus_stops[['route','tract']].isna().any(axis=1).mean(),\n",
    "))\n",
    "bus_stops = bus_stops.dropna(subset=['route','tract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since the bus ridership is at route granularity, not stop or point granularity,\n",
    "we need to aggregate to stop-level features to route.\n",
    "\n",
    "This is tricky since routes span miles across the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first thought is to \n",
    "compute the proportion of stops that exhibit a certain feature, ie proportion\n",
    "of stops near the United Center. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns out poorly scaled -- most routes\n",
    "have zero stops near the United Center, while those that do tend to have a very\n",
    "small proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of routes with ANY POI stops\n",
      "{'airport': 0.07, 'GUARANTEED RATE FIELD 1600m': 0.08, 'GUARANTEED RATE FIELD 400m': 0.03, 'GUARANTEED RATE FIELD 800m': 0.04, 'HYATT REGENCY MCCORMICK PLACE 1600m': 0.1, 'HYATT REGENCY MCCORMICK PLACE 400m': 0.01, 'HYATT REGENCY MCCORMICK PLACE 800m': 0.04, 'SOLDIER FIELD 1600m': 0.18, 'SOLDIER FIELD 400m': 0.03, 'SOLDIER FIELD 800m': 0.12, 'UNITED CENTER 1600m': 0.1, 'UNITED CENTER 400m': 0.03, 'UNITED CENTER 800m': 0.05, 'WRIGLEY FIELD 1600m': 0.12, 'WRIGLEY FIELD 400m': 0.01, 'WRIGLEY FIELD 800m': 0.06}\n"
     ]
    }
   ],
   "source": [
    "print('Proportion of routes with ANY POI stops')\n",
    "print((bus_stops_features > 0).mean().round(2).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of mean POI stops per route, given route serves ANY POI's:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport</th>\n",
       "      <th>GUARANTEED RATE FIELD 1600m</th>\n",
       "      <th>GUARANTEED RATE FIELD 400m</th>\n",
       "      <th>GUARANTEED RATE FIELD 800m</th>\n",
       "      <th>HYATT REGENCY MCCORMICK PLACE 1600m</th>\n",
       "      <th>HYATT REGENCY MCCORMICK PLACE 400m</th>\n",
       "      <th>HYATT REGENCY MCCORMICK PLACE 800m</th>\n",
       "      <th>SOLDIER FIELD 1600m</th>\n",
       "      <th>SOLDIER FIELD 400m</th>\n",
       "      <th>SOLDIER FIELD 800m</th>\n",
       "      <th>UNITED CENTER 1600m</th>\n",
       "      <th>UNITED CENTER 400m</th>\n",
       "      <th>UNITED CENTER 800m</th>\n",
       "      <th>WRIGLEY FIELD 1600m</th>\n",
       "      <th>WRIGLEY FIELD 400m</th>\n",
       "      <th>WRIGLEY FIELD 800m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.011403</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.008595</td>\n",
       "      <td>0.068063</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.019525</td>\n",
       "      <td>0.051464</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>0.014437</td>\n",
       "      <td>0.048024</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.009333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006883</td>\n",
       "      <td>0.120862</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.041513</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>0.127768</td>\n",
       "      <td>0.029103</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.107180</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>0.025111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.106918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airport  GUARANTEED RATE FIELD 1600m  GUARANTEED RATE FIELD 400m  \\\n",
       "count  65.000000                    65.000000                   65.000000   \n",
       "mean    0.002678                     0.043780                    0.002999   \n",
       "std     0.006883                     0.120862                    0.013013   \n",
       "min     0.000000                     0.000000                    0.000000   \n",
       "25%     0.000000                     0.000000                    0.000000   \n",
       "50%     0.000000                     0.000000                    0.000000   \n",
       "75%     0.000000                     0.000000                    0.000000   \n",
       "max     0.031250                     0.725490                    0.078431   \n",
       "\n",
       "       GUARANTEED RATE FIELD 800m  HYATT REGENCY MCCORMICK PLACE 1600m  \\\n",
       "count                   65.000000                            65.000000   \n",
       "mean                     0.011403                             0.029483   \n",
       "std                      0.041513                             0.074510   \n",
       "min                      0.000000                             0.000000   \n",
       "25%                      0.000000                             0.000000   \n",
       "50%                      0.000000                             0.000000   \n",
       "75%                      0.000000                             0.000000   \n",
       "max                      0.254902                             0.468750   \n",
       "\n",
       "       HYATT REGENCY MCCORMICK PLACE 400m  HYATT REGENCY MCCORMICK PLACE 800m  \\\n",
       "count                           65.000000                           65.000000   \n",
       "mean                             0.000734                            0.008595   \n",
       "std                              0.004235                            0.034018   \n",
       "min                              0.000000                            0.000000   \n",
       "25%                              0.000000                            0.000000   \n",
       "50%                              0.000000                            0.000000   \n",
       "75%                              0.000000                            0.000000   \n",
       "max                              0.028571                            0.234375   \n",
       "\n",
       "       SOLDIER FIELD 1600m  SOLDIER FIELD 400m  SOLDIER FIELD 800m  \\\n",
       "count            65.000000           65.000000           65.000000   \n",
       "mean              0.068063            0.005880            0.019525   \n",
       "std               0.127768            0.029103            0.063107   \n",
       "min               0.000000            0.000000            0.000000   \n",
       "25%               0.000000            0.000000            0.000000   \n",
       "50%               0.000000            0.000000            0.000000   \n",
       "75%               0.099010            0.000000            0.006289   \n",
       "max               0.600000            0.200000            0.400000   \n",
       "\n",
       "       UNITED CENTER 1600m  UNITED CENTER 400m  UNITED CENTER 800m  \\\n",
       "count            65.000000           65.000000           65.000000   \n",
       "mean              0.051464            0.003503            0.014437   \n",
       "std               0.123457            0.014706            0.044186   \n",
       "min               0.000000            0.000000            0.000000   \n",
       "25%               0.000000            0.000000            0.000000   \n",
       "50%               0.000000            0.000000            0.000000   \n",
       "75%               0.000000            0.000000            0.000000   \n",
       "max               0.666667            0.086614            0.222222   \n",
       "\n",
       "       WRIGLEY FIELD 1600m  WRIGLEY FIELD 400m  WRIGLEY FIELD 800m  \n",
       "count            65.000000           65.000000           65.000000  \n",
       "mean              0.048024            0.001447            0.009333  \n",
       "std               0.107180            0.008199            0.025111  \n",
       "min               0.000000            0.000000            0.000000  \n",
       "25%               0.000000            0.000000            0.000000  \n",
       "50%               0.000000            0.000000            0.000000  \n",
       "75%               0.029412            0.000000            0.000000  \n",
       "max               0.642857            0.050000            0.106918  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of mean POI stops per route, given route serves ANY POI's:\")\n",
    "bus_stops_features[(bus_stops_features>0).agg('any',axis=1)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we will interpret simply as binary whether or not the route serves the POI\n",
    "anywhere along the route, without taking into account the lenght of the route,\n",
    "nor ridership density along the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops_features = bus_stops.groupby('route')[feature_cols].any().astype(float)\n",
    "bus_routes = bus_routes.merge(bus_stops_features, how='left', left_on='route', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_rides = bus_routes.merge(bus_rides, on='route', how='right', validate='1:m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Rides + Tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXX:\n",
    "Not dropping anonymized observations (null tract/comm) because they still add to the non-spatial\n",
    "time series count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_tracts = tracts[feature_cols + ['geometry','centroid','geoid10']] \\\n",
    "                [lambda x: x['geoid10'].isin(uber_rides[uber_rides.unit == 'tract']['id'])] \\\n",
    "                .rename(columns={'geoid10':'id'})\n",
    "\n",
    "uber_comms = comms[feature_cols + ['geometry','centroid','comm_area']] \\\n",
    "                [lambda x: x['comm_area'].isin(uber_rides[uber_rides.unit == 'comm']['id'])]\\\n",
    "                .rename(columns={'comm_area':'id'})\n",
    "\n",
    "assert len(uber_rides.columns.intersection(uber_tracts.columns)) == 1\n",
    "assert len(uber_rides.columns.intersection(uber_comms.columns)) == 1\n",
    "\n",
    "uber_tract_rides = uber_tracts.merge(uber_rides[uber_rides.unit == 'tract'], how='right', on='id')\n",
    "uber_comm_rides = uber_comms.merge(uber_rides[uber_rides.unit == 'comm'], how='right', on='id')\n",
    "uber_rides = pd.concat([uber_tract_rides, uber_comm_rides], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides.to_parquet(uber_rides_out)\n",
    "bike_rides.to_parquet(bike_rides_out)\n",
    "train_rides.to_parquet(train_station_rides_out)\n",
    "train_line_rides.to_parquet(train_line_rides_out)\n",
    "bus_rides.to_parquet(bus_rides_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pickle\n",
    "from shapely import from_wkb\n",
    "from shapely.geometry import MultiPoint\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import contextily as cx\n",
    "\n",
    "from data.constants import (DATA_FOLDER, LOCAL_CRS, WORLD_CRS, WEB_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rides_in = os.path.join(DATA_FOLDER, \"deprecated\", \"bike_rides.geoparquet\")\n",
    "\n",
    "bike_rides_out = os.path.join(DATA_FOLDER, \"deprecated\", \"bike_rides_v2.geoparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOLERANCE = typical total street width = 2 sidewalks + 2 parking lanes + 2 traffic lanes\n",
    "#  source: https://www.chicago.gov/dam/city/depts/cdot/StreetandSitePlanDesignStandards407.pdf\n",
    "TOLERANCE = 66  # ft\n",
    "\n",
    "# We were using a much larger tolerance in the previous notebook, but if we're\n",
    "# doing this purely spatially, I think basically \"across the street\" is as far\n",
    "# as we want to merge things. City block ~= 0.1mi is another option, but there\n",
    "# are bus stops that are only one block apart so I can imagine divvies being \n",
    "# that close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rides = gpd.read_parquet(bike_rides_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rides = bike_rides.assign(\n",
    "    station_cluster_centroid = gpd.GeoSeries(bike_rides.station_cluster_centroid.apply(from_wkb), crs=WORLD_CRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for assertions later\n",
    "nobs = len(bike_rides)\n",
    "total_rides = bike_rides.filter(like='ride').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clusters = bike_rides.groupby('station_cluster_id')['geometry'].transform('nunique') > 1\n",
    "iso_clusters = bike_rides.groupby('station_cluster_id')['geometry'].transform('nunique') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've tried to reduce the data by different primary key sets.\n",
    "But maybe we should try directly spatially aggregating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Via buffer union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~2m\n",
    "lpoints = (bike_rides[['station_cluster_id','station_cluster_centroid','geometry']]\n",
    "            .set_geometry('station_cluster_centroid', crs=WORLD_CRS)\n",
    "            .drop(columns=['geometry'])\n",
    "            .rename_geometry('geometry')\n",
    "            .drop_duplicates()\n",
    "            .to_crs(LOCAL_CRS))\n",
    "\n",
    " # set radius as half tolerance ==> points unioned if distance <= 2 radii = 1 tol\n",
    "buffered = lpoints.buffer(TOLERANCE / 2)\n",
    "supercluster = buffered.union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert supercluster.geom_type == 'MultiPolygon'\n",
    "supercluster = gpd.GeoSeries(supercluster.geoms, crs=LOCAL_CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "This first step in the spatial merge will merge points that have different station names.\n",
    "The station names seem pretty good at separating what we should consider a station.\n",
    "The problem is that sometimes a station name has points really far away from it too.\n",
    "This is why station name -> centroid is still 1:m after attribute clustering.\n",
    "This next spatial clustering step runs pretty fast.\n",
    "If we add more checks in this loop it might make it slower.\n",
    "Instead we'll add checks in the next block when we actually compute the super centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~5m\n",
    "supercluster_file = os.path.join(DATA_FOLDER, \"deprecated\", 'superclusters.pickle')\n",
    "if not os.path.exists(supercluster_file):\n",
    "    cluster_to_supercluster = {}\n",
    "    sindex = supercluster.sindex\n",
    "    for i,pt in tqdm(lpoints.itertuples(index=False), total=len(lpoints)):\n",
    "        candidate_idx = sindex.query(pt, predicate='dwithin', distance=TOLERANCE*2)\n",
    "        candidates = supercluster[candidate_idx]\n",
    "        match_idx = candidate_idx[candidates.contains(pt)]\n",
    "        cluster_to_supercluster[i] = match_idx\n",
    "    with open(supercluster_file, 'wb') as f:\n",
    "        pickle.dump(cluster_to_supercluster, f, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(supercluster_file, 'rb') as f:\n",
    "        cluster_to_supercluster = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(map(lambda x: len(x)==1, cluster_to_supercluster.values()))\n",
    "supercluster_to_cluster = {}\n",
    "for k,v in cluster_to_supercluster.items():\n",
    "    supercluster_to_cluster.setdefault(int(v[0]), []).append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3918/3918 [01:59<00:00, 32.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Takes ~5m\n",
    "cluster_to_super_centroid = {}\n",
    "for sc, ids in tqdm(supercluster_to_cluster.items()):\n",
    "    sc_rides = bike_rides[bike_rides.station_cluster_id.isin(ids)]\n",
    "    # Further break down by station name because the data imprecision >> true station variance\n",
    "    for name, name_rides in sc_rides.groupby('station_name'):\n",
    "        name_ids = name_rides['station_cluster_id']\n",
    "        name_pts = name_rides['station_cluster_centroid']\n",
    "        super_centroid = MultiPoint(name_pts.to_crs(LOCAL_CRS).values).centroid\n",
    "        cluster_to_super_centroid |= {x: super_centroid for x in name_ids}\n",
    "bike_rides = bike_rides.assign(super_centroid = bike_rides.station_cluster_id.map(cluster_to_super_centroid))\n",
    "bike_rides = bike_rides.assign(super_centroid = gpd.GeoSeries(bike_rides.super_centroid, crs=LOCAL_CRS).to_crs(WORLD_CRS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rides.to_parquet(bike_rides_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
